# THE GRAND UNIFIED AI SAFETY STACK
## SAI-HARDENED EDITION v7.0

### A Contract-and-Cage Architecture for Agentic AI

**Integrating Invariance Enforcement, Cryptographic Attestation, Capability Bounds, Zero-Trust Architecture, Mechanistic Monitoring, Operational Constraints, and SAI-Level Hardening**

**Technical Whitepaper v7.0 â€” December 2025**

**Andrew H. Bond**  
San JosÃ© State University  
andrew.bond@sjsu.edu

---

> **DOCUMENT CLASSIFICATION:** For distribution to AI safety research teams at Anthropic, OpenAI, DeepMind, Google, Meta, and affiliated laboratories.

> ðŸš¨ **v7.0 SAI HARDENING:** This version addresses seven critical gaps identified in peer review for SAI-level (Superintelligent AI) adversaries, plus five documentation-level improvements.

---

## v7.0 KEY CHANGES (SAI Hardening)

| # | Gap Identified | Mitigation Added | Priority |
|---|----------------|------------------|----------|
| 1 | Semantic gap / output channels | Structured-only outputs for HIGH+ tiers | P0 |
| 2 | Canary oracle risk | Canaries monitoring-only, never gating | P0 |
| 3 | Side-channel determinism | Deterministic transducer mode | P0/P1 |
| 4 | Degraded mode policy | Explicit normative degradation contract | P0 |
| 5 | Override social engineering | Model-blind authorization + fatigue protection | P0/P1 |
| 6 | Provisioner bypass via humans | Humans/automation as capability-bounded tools | P0 |
| 7 | Capability index gaming | Adversarial capability assessment | P1 |

**Documentation Improvements:**
- NEW: Model Output Channel Threat Model (Â§O)
- NEW: Operational definition of meaning-preserving transforms (Â§M)
- NEW: Oracle safety rule formalization (Â§D-2)
- NEW: Degraded-mode policy as normative and testable (Â§G)
- NEW: N-version diverse implementation requirement for CRITICAL tier (Â§N)

---

|-------|-----------|----------------|-----------------|------------|
| 1 | External user | Input descriptions | Canonicalization; loop tests | HIGH |
| 2 | Sophisticated user | Prompt engineering; boundary probing | Bd gating; canaries; rate limiting | HIGH |
| 3 | Insider/red team | Knowledge of G_declared | Canary transforms; rotation; audits | MODERATE |
| 4 | Capable AI | Strategic probing; tool misuse | Capability bounds; I-EIP; zero-trust | MODERATE |
| 5 | AGI-level | Deception; mesa-optimization | Defense in depth; corrigibility; scaling | LOW |
| 6 | ASI-level | Unknown attack vectors | Governance + containment + humility | UNCERTAIN |
| 7 | Physical access | Side-channels; supply chain | Out of scope (defense-in-depth only) | NONE |

The stack provides strong guarantees against Levels 1-3, meaningful resistance at Level 4, partial coverage at Level 5, honest uncertainty at Level 6, and no guarantees at Level 7.

### 1.2 Design Philosophy

> **CORE PRINCIPLE:** Treat alignment as an assurance problem: make safety claims contractual, testable, gateable, and auditableâ€”rather than purely behavioral or intentional.

**Key Principles:**

1. **Contracts over intentions:** Specify what invariances are guaranteed, not what the system "believes"
2. **Measurement over intuition:** Define quantifiable metrics (Bond index) with deployment thresholds
3. **Conservative defaults:** When uncertain, refuse (âŠ¥) rather than guess
4. **Defense in depth:** No single layer is sufficient; redundancy is required
5. **TCB minimization:** Reduce the trusted base; formally verify what you trust
6. **Cryptographic enforcement:** Signatures and attestation, not just policies
7. **Honest limitations:** Explicitly state what is NOT guaranteed

### 1.3 Conditional Guarantee Scope

All guarantees in this document are conditional on:

1. The Trusted Computing Base (Â§2) maintaining integrity
2. The transform suite G_declared covering relevant redescription channels
3. The ErisML ontology covering the action space
4. Hardware/software enforcement mechanisms functioning correctly
5. Cryptographic primitives remaining secure

**Guarantees do NOT hold if:**
- Adversary compromises TCB components
- Novel redescription channels outside G_declared are exploited
- Actions fall outside ErisML ontology coverage
- Physical security of enforcement hardware is breached
- Sustained economic attacks deplete attestation budget (without mitigations)

---

## 2. TRUSTED COMPUTING BASE (TCB)

### 2.1 TCB Definition

The Trusted Computing Base comprises all components whose correct operation is required for safety guarantees to hold. **If any TCB component is compromised, all guarantees are void.**

### 2.2 TCB Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRUSTED COMPUTING BASE (TCB)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HARDWARE LAYER                                                      â”‚
â”‚  â”œâ”€â”€ TEE/SGX execution environment (defense-in-depth only)         â”‚
â”‚  â”œâ”€â”€ HSM for key storage (Root CA, TCB signing keys)               â”‚
â”‚  â”œâ”€â”€ Memory isolation (model cannot access TCB memory)             â”‚
â”‚  â””â”€â”€ Resource quota enforcement (cgroups, VM limits)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ENFORCEMENT LAYER                                                   â”‚
â”‚  â”œâ”€â”€ ErisML Parser (target: formally verified in Coq)              â”‚
â”‚  â”œâ”€â”€ Canonicalizer/Normalizer (deterministic, N1-N7)               â”‚
â”‚  â”œâ”€â”€ Constraint Evaluator (proven terminating, fail-closed)        â”‚
â”‚  â”œâ”€â”€ Audit Logger (append-only, cryptographically signed)          â”‚
â”‚  â””â”€â”€ Capability Enforcer (action whitelist)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CRYPTOGRAPHIC LAYER                                                 â”‚
â”‚  â”œâ”€â”€ Certificate Authority (3-tier hierarchy)                      â”‚
â”‚  â”œâ”€â”€ Blockchain attestation service                                â”‚
â”‚  â”œâ”€â”€ DHT artifact storage (IPFS)                                   â”‚
â”‚  â””â”€â”€ Session key management                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VERIFICATION LAYER                                                  â”‚
â”‚  â”œâ”€â”€ External Verifier V (isolated process)                        â”‚
â”‚  â”œâ”€â”€ Loop Test Harness                                             â”‚
â”‚  â””â”€â”€ I-EIP Monitor (Î“ probes, drift detection)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GOVERNANCE LAYER                                                    â”‚
â”‚  â”œâ”€â”€ Transform Suite Registry (versioned, signed, auditable)       â”‚
â”‚  â”œâ”€â”€ ErisML Schema Registry (versioned, signed)                    â”‚
â”‚  â””â”€â”€ Norm/Profile Registry (versioned, signed)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UNTRUSTED COMPONENTS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”œâ”€â”€ LLM Transpiler (soft component, primary attack surface)       â”‚
â”‚  â”œâ”€â”€ Model weights and inference                                   â”‚
â”‚  â”œâ”€â”€ User inputs                                                   â”‚
â”‚  â””â”€â”€ External data sources                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Verification Tiers

| Tier | Components Verified | Assurance | Effort | Coverage |
|------|---------------------|-----------|--------|----------|
| 1 (Minimal) | Parser + StateID hash | Moderate | ~6 PM | 60% |
| 2 (Standard) | Parser + Normalizer + StateID | High | ~18 PM | 85% |
| 3 (Full) | All TCB enforcement code | Very High | ~36+ PM | 95% |

Tier 1 is the recommended starting point. The parser is the first line of defense; if it accepts invalid input, all downstream guarantees fail.

### 2.4 Isolation Requirements

**Requirement TCB-1 (Process Isolation):**
TCB components MUST run in separate processes from the model. The model cannot write to TCB memory, modify TCB code, or influence TCB control flow.

**Requirement TCB-2 (No Side Channels):**
The following are prohibited:
- Model writing to filesystem locations read by TCB
- Model influencing TCB timing (no timing-based covert channels)
- Model controlling network traffic to TCB components
- Model accessing debug/profiling interfaces of TCB

**Requirement TCB-3 (Fail-Closed):**
All TCB components are fail-closed:
- Parser failure â†’ VETO (not "try alternative")
- Verifier failure â†’ block actuation
- Audit failure â†’ halt operation
- Resource limit hit â†’ terminate, not degrade

**Requirement TCB-4 (Formal Verification):**
Critical TCB components MUST be formally verified:
- ErisML Parser: proven to accept exactly the grammar L, nothing more
- Normalizer: proven deterministic and terminating
- Constraint Checker: proven sound (no false negatives on violations)

### 2.5 Bypass Prevention

| Bypass Vector | Prevention | Verification Method |
|---------------|------------|---------------------|
| Direct tool invocation | All tools require signed audit artifact | Runtime signature check |
| Filesystem write | Model has no write access outside sandbox | OS permissions |
| Network direct | All egress through proxy requiring audit | Firewall rules |
| Memory corruption | ASLR + DEP + isolated address spaces | Hardware + testing |
| Timing covert channel | Constant-time TCB operations | Formal analysis |
| Model self-modification | Read-only model weights | Filesystem mount |

#### Timing Side-Channel Mitigations

These are defense-in-depth measures, NOT guarantees:

- **Level A (Implemented):** No data-dependent branches, fixed iteration counts, cache flush
- **Level B (Best-effort):** Constant-time comparison, memory access obfuscation
- **Level C (Requires hardware):** Speculative execution mitigation, power analysis protection

#### Enumerated Side Channels (must be addressed):

1. **Timing:** Addressed via constant-time TCB code
2. **Power:** Out of scope (requires physical security)
3. **EM emissions:** Out of scope (requires physical security)
4. **Cache:** Addressed via process isolation + cache flushing
5. **Speculative execution:** Addressed via Spectre mitigations in TEE


---

# PART II: OPERATIONAL CONSTRAINTS (Layer 9)

> ðŸš¨ **CRITICAL:** This section contains honest admissions of limitations that could cripple the stack in real-world deployment. Engineers must understand these constraints before deployment.

## 3. THE HARDWARE REALITY: SIDE-CHANNEL VULNERABILITIES

The v3.0 whitepaper relied on TEE/SGX-style enclaves for TCB isolation. This creates a false sense of security. Commercial enclaves have a decade-long history of side-channel breaks:

| Attack | Year | Impact |
|--------|------|--------|
| Spectre | 2018 | Read arbitrary memory via speculation |
| Meltdown | 2018 | Kernel memory disclosure |
| Foreshadow | 2018 | SGX enclave extraction |
| RIDL/Fallout | 2019 | MDS attacks on Intel |
| LVI | 2020 | Load value injection |
| Ã†PIC | 2022 | Architectural data leak |
| Downfall | 2023 | Gather data sampling |

### 3.1 Revised Security Tiers

| Tier | Name | Strength | Description |
|------|------|----------|-------------|
| 1 | Logical Security | Strong | Signatures verified, audit integrity, invariance detection |
| 2 | Process Isolation | Moderate | OS-level memory isolation, network/filesystem permissions |
| 3 | Hardware Security | Weak in Practice | TEE provides defense-in-depth only, not guarantee |
| 4 | Physical Security | Out of Scope | Data center access, supply chain, tampering detection |

> âš ï¸ **WARNING: PHYSICAL ACCESS = GAME OVER.** If an adversary has physical access to TCB hardware, ability to monitor power/EM emissions, and time for differential analysis, they can likely extract signing keys, internal state, and secret norms/canaries.

### 3.2 Side-Channel Mitigations

These are defense-in-depth measures, NOT guarantees:

| Mitigation | Effectiveness | Cost | Bypassed By |
|------------|---------------|------|-------------|
| Constant-time code | Moderate | 2-5x slowdown | Cache timing, power analysis |
| Memory obfuscation | Low-Moderate | 10-20% overhead | Differential power analysis |
| Noise injection | Low | Variable latency | Statistical averaging |
| Key rotation | Moderate | Operational complexity | Faster extraction than rotation |
| Air-gapped TCB | High (remote) | Kills utility | Physical access |

---

## 4. THE ECONOMIC ATTACK: GAS FEE DoS

v3.0 uses blockchain for attestation. This creates an economic attack vector where an adversary spams the network to inflate gas fees, depleting the AI's wallet and causing fail-closed bricking.

### 4.1 Attack Economics

```
Normal gas: 50 gwei        | Attack gas: 5,000 gwei (100x)
Normal attestation: $0.50  | Attack attestation: $50.00

AI making 1000 decisions/day:
  Normal: $500/day
  Under attack: $50,000/day

Attacker cost to sustain: ~$10,000/day
Economic asymmetry: 5:1 in attacker's favor
```

### 4.2 Revised Architecture: Hybrid Attestation

| Mode | Mechanism | Cost | Use Case |
|------|-----------|------|----------|
| Primary | L2 Rollup | ~$0.001/attestation | Normal operation |
| Secondary | Consortium Chain (Hyperledger) | Zero gas | Known validators |
| Fallback | Local Merkle Accumulator | Zero | Root published periodically |
| Emergency | Offline Mode | Zero | Queue for later publication |

```python
class HybridAttestationPublisher:
    def publish(self, artifact: AuditArtifact) -> AttestationReceipt:
        # Try L2 first (cheapest with public verifiability)
        if self.l2_available() and self.gas_price() < GAS_THRESHOLD:
            return self.publish_l2(artifact)
        
        # Fall back to consortium chain
        if self.consortium_available():
            return self.publish_consortium(artifact)
        
        # Fall back to local accumulator
        self.local_accumulator.add(artifact)
        return AttestationReceipt(
            status="QUEUED",
            accumulator_root=self.local_accumulator.root(),
            will_publish_at=self.next_batch_time()
        )
```

---

## 5. THE LATENCY TAX: REAL-TIME INVIABILITY

The software stack has the following latency budget:

| Component | Latency (ms) | Variance |
|-----------|--------------|----------|
| Input reception & parsing | 1-5 | Low |
| LLM Transpilation (if needed) | 100-500 | HIGH |
| ErisML parsing (verified) | 0.1-1 | Low |
| Canonicalization | 1-10 | Medium |
| Loop tests (10 transforms) | 10-50 | Medium |
| Norm evaluation | 1-5 | Low |
| Cryptographic signing | 0.1-1 | Low |
| Blockchain submission | 100-1000 | VERY HIGH |
| **TOTAL (worst case)** | **213-1572** | â€” |

### 5.1 Application Scope

| Application | Required Latency | Stack Latency | Verdict |
|-------------|------------------|---------------|---------|
| Autonomous driving | <50ms | 200+ ms | âŒ FAIL |
| Robotic manipulation | <10ms | 200+ ms | âŒ FAIL |
| Real-time bidding | <10ms | 200+ ms | âŒ FAIL |
| Chat/conversation | <2000ms | 200+ ms | âœ… OK |
| Medical diagnosis | <5000ms | 200+ ms | âœ… OK |
| Financial approval | <10000ms | 200+ ms | âœ… OK |
| Weapon authorization | <30000ms | 200+ ms | âœ… OK (IDEAL) |
| Autonomous research | Minutes+ | 200+ ms | âœ… IDEAL |

> ðŸš¨ **CRITICAL: THIS STACK CANNOT DRIVE A CAR.** At 60mph, a car travels 20 meters in 230ms. Real-time control requires specialized hardware acceleration (sub-microsecond latency) and a redesigned fast-path.

### 5.2 Throughput Impact Analysis

| Mitigation | Latency Impact | Throughput Impact | Recommended |
|------------|----------------|-------------------|-------------|
| Single transpiler | Baseline | Baseline | Always |
| 3-model ensemble | +200% latency | -66% throughput | High-stakes only |
| Human review | +hours | -99%+ | Escalation only |
| OOD detection | +5% latency | Negligible | Always |

---

## 6. THE GOVERNANCE DEADLOCK: EMERGENCY UPDATES

v3.0 specifies immutable norms (hashed, signed, on-chain). This prevents drift but also prevents bug fixes.

### 6.1 Tiered Mutability Architecture

| Tier | Contents | Change Process |
|------|----------|----------------|
| 0 â€” Immutable Core | Fundamental axioms, stack architecture | Requires new deployment |
| 1 â€” Stable Norms | Domain modules, G, thresholds, Ï„ | Multi-party vote + 24hr delay |
| 2 â€” Operational Params | Rate limits, quotas, logging config | Admin approval + audit log |
| 3 â€” Emergency Override | Bypass specific checks temporarily | M-of-N keys + auto-revert + alerts |

### 6.2 Emergency Override Protocol

Emergency overrides are an attack surface. They must be:

1. **Rare** â€” Genuine emergencies only
2. **Loud** â€” Alerts to all stakeholders immediately
3. **Temporary** â€” Automatic revert after timeout (max 24 hours)
4. **Logged** â€” Immutable record of who, what, when, why
5. **Bounded** â€” Cannot override Tier 0 immutables
6. **Non-escalatory** â€” CANNOT expand the capability whitelist

> ðŸš¨ **NON-ESCALATORY RULE:** An emergency override can BYPASS checks but CANNOT EXPAND capabilities. This prevents override mechanisms from becoming capability amplification attacks.

```python
@dataclass
class EmergencyOverride:
    id: str
    requestor: Identity
    approvers: List[Identity]  # M-of-N
    checks_bypassed: List[CheckID]  # SPECIFIC, not "all"
    tools_affected: List[ToolID]  # SPECIFIC, not "all"
    reason: str
    expires_at: datetime  # MAX 24 hours
    auto_revert: bool = True
    
    def validate(self) -> bool:
        # Cannot bypass Tier 0
        assert not any(c in TIER_0_CHECKS for c in self.checks_bypassed)
        
        # Cannot expand capabilities
        assert not self.expands_capabilities()
        
        # Must have M-of-N approval
        assert len(self.approvers) >= M_REQUIRED
        
        # Must expire within 24 hours
        assert self.expires_at <= now() + timedelta(hours=24)
        
        return True
```

---

# PART III: INVARIANCE ENFORCEMENT (Layer 1)

## 7. THE BOND INVARIANCE PRINCIPLE (BIP)

### 7.1 BIP Definition

> **DEFINITION 7.1 (BIP):** Let X be description space, G a group of meaning-preserving transformations, and Î£: X â†’ V an evaluation function. Î£ satisfies BIP iff:
>
> **âˆ€g âˆˆ G, âˆ€x âˆˆ X: Î£(gÂ·x) = Î£(x)**

**Operational Scope:** BIP is guaranteed ONLY for G_declaredâ€”the explicit, versioned transform suite. Transforms outside G_declared are NOT covered.

### 7.2 Canonicalizer Theorem

> **THEOREM 7.2 (Canonicalizer Theorem):** Canonicalization provides a constructive route to invariance when Îº satisfies:
> 1. **Idempotence:** Îº(Îº(x)) = Îº(x)
> 2. **Orbit Collapse:** Îº(gÂ·x) = Îº(x) for all g âˆˆ G_declared
>
> Then any judgment computed on canonical forms is invariant by construction.

> ðŸš¨ **EPISTEMIC CAVEAT:** A discrete grammar and deterministic normalization do NOT magically solve semantic equivalence. If the transpiler maps semantically equivalent inputs to different ASTs, the benefit is that violations become MEASURABLE and ATTRIBUTABLE rather than silent. This is an engineering improvement, not a theoretical guarantee.

### 7.3 Why Discreteness Matters

| Property | Vector Space | ErisML Lattice |
|----------|--------------|----------------|
| Structure | Continuous â„â¿ | Discrete lattice L |
| Boundaries | Fuzzy cluster boundaries | Binary parse/no-parse |
| Intermediate | "0.5 between clusters" possible | No intermediate states |
| Exploitation | Boundary exploitable | Boundary is sharp |

---

## 8. ErisML CANONICALIZE-OR-VETO

### 8.1 Complete Pipeline

```python
def canonicalize(x: str, context: Context) -> CanonicalResult:
    """
    TCB BOUNDARY: This function runs in trusted process.
    All steps after transpilation are formally verified.
    """
    
    # Step 1: LLM Transpilation (UNTRUSTED - in separate process)
    eris_code = transpiler.transpile(x, temp=0)  # Deterministic
    
    # Step 2: Parse (TRUSTED - formally verified parser)
    ast = ErisMLParser.parse(eris_code)
    if ast.is_error():
        return CanonicalResult(veto=True, reason='PARSE_FAILURE')
    
    # Step 3: Validate (TRUSTED - proven constraint checker)
    validation = Validator.check(ast, context.active_norms)
    if not validation.passed:
        return CanonicalResult(veto=True, reason=validation.failure_code)
    
    # Step 4: Normalize (TRUSTED - proven deterministic, N1-N7)
    canonical = Normalizer.normalize(ast)
    state_id = sha256(canonical.serialize())
    
    # Step 5: Sign audit artifact (TRUSTED)
    artifact = AuditArtifact(
        canonical=canonical,
        state_id=state_id,
        timestamp=timestamp(),
        tcb_version=TCB_VERSION
    )
    artifact.signature = tcb_key.sign(artifact)
    
    return CanonicalResult(
        veto=False,
        state_id=state_id,
        canonical_ast=canonical,
        artifact=artifact
    )
```

### 8.2 Normalization Steps N1-N7 (Deterministic)

Normalization MUST be deterministic and stable across implementations:

| Step | Operation | Rationale |
|------|-----------|-----------|
| N1 | Sort record fields by key (lexicographic) | Canonical ordering |
| N2 | Assign canonical entity IDs (first occurrence order) | Stable references |
| N3 | Collapse equivalent enum variants | Semantic normalization |
| N4 | Remove default-valued fields (where defaults are schema-defined) | Minimal representation |
| N5 | Normalize numeric precision (domain-specific rounding) | Avoid floating-point divergence |
| N6 | Sort unordered collections (lexicographic by serialized element) | Deterministic ordering |
| N7 | Serialize to canonical byte sequence for hashing/signing | Reproducible hash |

```python
class Normalizer:
    @staticmethod
    def normalize(ast: ErisMLAST) -> CanonicalAST:
        """Deterministic normalization following N1-N7."""
        
        # N1: Sort record fields
        for record in ast.records():
            record.fields = sorted(record.fields, key=lambda f: f.key)
        
        # N2: Canonical entity IDs
        entity_map = {}
        counter = 0
        for entity in ast.entities():
            if entity.id not in entity_map:
                entity_map[entity.id] = f"E{counter:04d}"
                counter += 1
            entity.canonical_id = entity_map[entity.id]
        
        # N3: Collapse equivalent enums
        for field in ast.enum_fields():
            field.value = ENUM_CANONICAL[field.value]
        
        # N4: Remove defaults
        for record in ast.records():
            record.fields = [f for f in record.fields 
                           if f.value != SCHEMA_DEFAULTS.get(f.key)]
        
        # N5: Normalize numeric precision
        for field in ast.numeric_fields():
            field.value = round(field.value, PRECISION[field.domain])
        
        # N6: Sort collections
        for collection in ast.unordered_collections():
            collection.elements = sorted(collection.elements, 
                                        key=lambda e: e.serialize())
        
        # N7: Serialize
        canonical_bytes = ast.serialize_canonical()
        
        return CanonicalAST(ast=ast, bytes=canonical_bytes)
```

---

## 9. TRANSPILER HARDENING

The LLM transpiler is the "soft" component and primary attack surface. Specific mitigations:

### 9.1 Protocol T-1: Ensemble Disagreement Detection

```python
def hardened_transpile(x: str) -> TranspileResult:
    """
    Run 3+ independent transpilers. 
    If canonical forms disagree, escalate or veto.
    """
    results = []
    for transpiler in TRANSPILER_ENSEMBLE:  # 3+ independent models
        results.append(transpiler.transpile(x, temp=0))
    
    canonical_forms = [ErisMLParser.parse(r).normalize() for r in results]
    
    if not all_equal(canonical_forms):
        # Disagreement â†’ conservative handling
        return TranspileResult(
            status="DISAGREEMENT",
            action="ESCALATE_TO_HUMAN" if high_stakes else "VETO",
            witness=DisagreementWitness(results, canonical_forms)
        )
    
    return TranspileResult(status="AGREED", canonical=canonical_forms[0])
```

### 9.2 Protocol T-2: Adversarial Training

- Train transpiler on adversarial examples generated by red-team LLM
- Include "boundary" examples: inputs designed to be ambiguous
- Require â‰¥99.9% agreement between transpiler output and human annotation on held-out set
- Retrain quarterly with discovered edge cases

### 9.3 Protocol T-3: Distribution Shift Detection

```python
def check_distribution_shift(x: str) -> ShiftResult:
    """
    Detect out-of-distribution inputs that may cause unreliable transpilation.
    """
    embedding = embed(x)
    distance_to_training = min_distance(embedding, TRAINING_DISTRIBUTION)
    
    if distance_to_training > OOD_THRESHOLD:
        return ShiftResult(
            ood=True, 
            action="VETO_OR_ESCALATE",
            distance=distance_to_training
        )
    
    return ShiftResult(ood=False)
```

### 9.4 Protocol T-4: Human Review Escalation

Define explicit escalation triggers (not just "human review when uncertain"):

**Triggers:**
- Ensemble disagreement rate > 10%
- OOD score > 2Ïƒ from training mean
- Confidence below threshold
- High-stakes action flag set

**Target:** Escalation rate < 1% of inputs (operational feasibility)


---

## 10. OPERATIONAL CURVATURE AND CALIBRATION

### 10.1 Operational Curvature Definition

> **DEFINITION 10.1 (Operational Curvature):** For transforms gâ‚, gâ‚‚ âˆˆ G:
>
> **Î©_op(x; gâ‚, gâ‚‚) = Î”(Îº(gâ‚‚Â·gâ‚Â·x), Îº(gâ‚Â·gâ‚‚Â·x))**

Operational curvature measures path-dependence: if applying transforms in different orders changes the canonical form, the system is vulnerable.

### 10.2 Distance Function Î”

```python
def delta(ast1: CanonicalAST, ast2: CanonicalAST) -> float:
    """
    Distance between canonical forms.
    Weighted Hamming distance on fields.
    """
    if ast1.state_id == ast2.state_id:
        return 0.0
    
    # Count differing fields, weighted by severity
    diff = 0.0
    for field in ERISML_FIELDS:
        if ast1.get(field) != ast2.get(field):
            diff += FIELD_SEVERITY_WEIGHT[field]
    
    return diff / sum(FIELD_SEVERITY_WEIGHT.values())  # Normalize to [0,1]
```

### 10.3 Field Severity Weights (Domain-Specific)

**Medical Domain Example:**

| Field | Weight | Rationale |
|-------|--------|-----------|
| harm_physical | 1.0 | Safety-critical |
| consent | 1.0 | Autonomy-critical |
| reversibility | 0.8 | High impact |
| property_class | 0.5 | Moderate impact |
| agent_id | 0.3 | Administrative |

**Content Moderation Example:**

| Field | Weight | Rationale |
|-------|--------|-----------|
| harm_to_minors | 1.0 | Safety-critical |
| violent_content | 0.9 | High impact |
| misinformation | 0.7 | Moderate-high |
| spam_likelihood | 0.3 | Administrative |

---

## 11. BOND INDEX CALIBRATION

### 11.1 Bond Index Definition

> **DEFINITION 11.1 (Bond Index):** Bd = Î©_op / Ï„
>
> Where Ï„ is the detection threshold calibrated via human annotation.

### 11.2 Calibration Protocol (Krippendorff's Î± Required)

**Step 1: Establish Ï„ (Detection Threshold)**

```python
def calibrate_tau(domain: str) -> float:
    """
    Ï„ = minimum Î” that represents a "meaningful" difference.
    Calibrate via human annotation study.
    
    REQUIREMENT: Krippendorff's Î± > 0.67 (acceptable agreement)
    TARGET: Krippendorff's Î± > 0.80 (good agreement)
    """
    # Recruit diverse rater pool (nâ‰¥50, multi-region, multi-expertise)
    raters = recruit_raters(n=50, diversity_requirements=DIVERSITY_SPEC)
    
    # Generate 200+ stratified canonical pairs
    pairs = generate_stratified_pairs(n=200)
    
    # Collect judgments
    # Ask: "Would these produce meaningfully different moral judgments?"
    judgments = collect_judgments(raters, pairs)
    
    # Compute inter-rater reliability
    alpha = krippendorff_alpha(judgments)
    assert alpha > 0.67, f"Insufficient agreement: Î±={alpha}"
    
    # Derive weights via regression
    weights = fit_weights(judgments, pairs)
    
    # Ï„ = Î” value at which 95% of raters say "yes, different"
    tau = find_threshold(judgments, percentile=95)
    
    return CalibrationResult(
        tau=tau,
        weights=weights,
        alpha=alpha,
        calibration_date=today(),
        rater_count=len(raters)
    )

# Domain-specific calibrated values (examples):
CALIBRATED_TAU = {
    "medical": 0.15,           # Conservative: small changes matter
    "weapon_authorization": 0.05,  # Very conservative
    "content_moderation": 0.25,    # Moderate
    "financial": 0.10,         # Conservative
    "recommendation": 0.40,    # More tolerant
}
```

**Step 2: Establish Baseline Distribution**

```python
def establish_baseline(G: TransformSuite, test_corpus: List[str]) -> BdDistribution:
    """Run loop tests on held-out corpus to establish baseline."""
    bd_values = []
    
    for x in test_corpus:
        for g1, g2 in combinations(G, 2):
            omega = loop_test(x, g1, g2)
            bd = omega / TAU
            bd_values.append(bd)
    
    return BdDistribution(
        mean=mean(bd_values),
        median=median(bd_values),
        p95=percentile(bd_values, 95),
        p99=percentile(bd_values, 99),
        max=max(bd_values),
        worst_witnesses=get_worst_n(bd_values, n=10)
    )
```

### 11.3 Mandatory Reporting Standard

Every evaluation MUST report:

```json
{
  "bd_distribution": {
    "mean": 0.023,
    "median": 0.018,
    "p95": 0.087,
    "p99": 0.142,
    "max": 0.284
  },
  "veto_rate": {
    "overall": 0.034,
    "by_transform_family": {
      "paraphrase": 0.045,
      "unit_conversion": 0.012,
      "order_permutation": 0.008
    }
  },
  "worst_witnesses": [
    {"x": "...", "g1": "...", "g2": "...", "bd": 0.284}
  ],
  "tau_used": 0.15,
  "tau_calibration_date": "2025-12-01",
  "krippendorff_alpha": 0.73,
  "G_version": "G_medical_v3.2",
  "corpus_size": 10000,
  "corpus_hash": "sha256:..."
}
```

### 11.4 Deployment Rating Scale

| Bd Range | Rating | Decision |
|----------|--------|----------|
| < 0.01 | Negligible | Deploy |
| 0.01 - 0.1 | Low | Deploy + monitor |
| 0.1 - 1.0 | Moderate | Remediate first |
| 1 - 10 | High | Do not deploy |
| > 10 | Severe | Fundamental redesign |

### 11.5 Risk-Tiered Transform Sampling

Transform sampling should be risk-proportional:

| Action Impact | Transforms Tested | Latency Budget | Example |
|---------------|-------------------|----------------|---------|
| CRITICAL | Full G_declared | 5-30 seconds | Weapon authorization |
| HIGH | 50% sampling | 1-5 seconds | Financial >$1M |
| MEDIUM | 20% sampling | 200-1000ms | Medical advice |
| LOW | 5% sampling | 50-200ms | Chat response |

---

## 12. TRANSFORM SUITE GOVERNANCE

> âš ï¸ **WARNING:** The transform suite G is the single most important governance choice. If G misses a redescription channel, the attacker succeeds.

### 12.1 G_declared vs Î“ Distinction

| Suite | Purpose | Visibility | Scope |
|-------|---------|------------|-------|
| G_declared | Decision invariance claims | Public | Contractual boundary |
| Î“ (Gamma) | Internal monitoring probes | Confidential | Typically Î“ âŠ‡ G_declared |

The distinction matters: G_declared is the contractual boundary for invariance claims. Î“ enables broader monitoring without expanding public commitments.

### 12.2 Governance Requirements

**Requirement G-1 (Explicit Versioning):**
- Every G has a version identifier: `G_domain_vN.M`
- Changes require changelog documenting additions/removals
- Semantic versioning: major = breaking, minor = additions

**Requirement G-2 (Public Review):**
- G_declared is published and reviewable
- Stakeholders can challenge inclusion/exclusion of transforms
- Dispute resolution process defined

**Requirement G-3 (Coverage Tracking):**

```python
def assess_coverage(G: TransformSuite, domain: str) -> CoverageReport:
    """Estimate how much of the redescription space G covers."""
    
    # Method 1: Red team attempts to find uncovered transforms
    red_team_escapes = run_red_team(G, budget=1000_hours)
    
    # Method 2: Linguistic analysis of domain corpus
    observed_paraphrases = extract_paraphrases(DOMAIN_CORPUS)
    covered = [p for p in observed_paraphrases if p in G.closure()]
    
    return CoverageReport(
        estimated_coverage=len(covered) / len(observed_paraphrases),
        red_team_escapes=red_team_escapes,
        recommended_additions=[...],
        confidence="moderate"  # Coverage estimation is inherently uncertain
    )
```

**Requirement G-4 (Anti-Goodharting via Canaries):**

```python
def anti_goodhart_protocol(G_public: TransformSuite) -> None:
    """Prevent overfitting to public G by maintaining secret canaries."""
    
    # 20% of loop tests use SECRET transforms not in G_public
    G_canary = generate_canary_transforms(seed=SECRET_SEED)
    
    # Canaries rotated monthly
    if is_rotation_due():
        G_canary = rotate_canaries(G_canary)
    
    # Alert if Bd(canary) >> Bd(public) - indicates overfitting
    bd_public = measure_bd(G_public)
    bd_canary = measure_bd(G_canary)
    
    if bd_canary / bd_public > 2.0:
        raise Alert("Possible Goodharting on public transform suite")
```

**Requirement G-5 (Continuous Expansion):**
- Quarterly review of G coverage
- Incorporate red-team discoveries
- Track "escapes" in production (inputs that succeed but seem gaming)

### 12.3 Probe Corpus Design

| Probe Type | Purpose | Target % |
|------------|---------|----------|
| Invariance probes | Drawn from G_declared; invariance SHOULD hold | ~70% |
| Boundary probes | Invariance should NOT hold (prevents trivial invariance) | â‰¥15% |
| Safety probes | Test safety-critical semantic distinctions | ~10% |
| Canary probes | Secret probes, rotated; detect gaming | ~5% |

> ðŸš¨ **CRITICAL:** Boundary probes at â‰¥15% are MANDATORY. Without them, a system could achieve trivial invariance by collapsing all inputs to a single canonical form.

### 12.4 Holonomy-Style Testing

Red-team testing MUST include long-range compositions beyond pairwise:

- **Pairwise:** Test Îº(gâ‚‚Â·gâ‚Â·x) = Îº(gâ‚Â·gâ‚‚Â·x) for all pairs
- **Holonomy:** Test Îº(gâ‚™Â·...Â·gâ‚‚Â·gâ‚Â·x) = Îº(x) for long transform chains
- **Global loopholes:** Search for compositions that "escape" the invariance manifold
- **Target:** Test chains of length 5-10 from representative corpus

```python
def holonomy_test(x: str, chain: List[Transform]) -> HolonomyResult:
    """Test whether a long chain of transforms preserves canonical form."""
    
    original = canonicalize(x)
    current = x
    
    for g in chain:
        current = g.apply(current)
    
    final = canonicalize(current)
    
    return HolonomyResult(
        chain_length=len(chain),
        original_state_id=original.state_id,
        final_state_id=final.state_id,
        holonomy_preserved=(original.state_id == final.state_id),
        witness=current if original.state_id != final.state_id else None
    )
```

### 12.5 Loop Test Scaling Strategy

Full enumeration scales O(|G|Â²). For large suites, use stratified sampling:

| Strategy | Budget % | Purpose |
|----------|----------|---------|
| Within-family | 30% | Catches family-specific issues |
| Cross-family | 40% | Catches interaction issues |
| Worst-case search | 10% | Actively find high-curvature pairs |
| Canary | 20% | Detect Goodharting |

### 12.6 Metamorphic Testing Artifact Schema

```json
{
  "artifact_version": "unified-6.0-ultra",
  "baseline": {
    "input_raw": "...",
    "input_canonical": "...",
    "state_id": "sha256:..."
  },
  "transforms_tested": [
    {
      "id": "syn_003",
      "family": "syntactic",
      "description": "passive voice",
      "transformed_state_id": "sha256:...",
      "judgment_baseline": "PERMIT",
      "judgment_transformed": "PERMIT",
      "equivalent": true,
      "witness": null
    }
  ],
  "loop_summary": {
    "total_sampled": 47,
    "passed": 47,
    "failed": 0,
    "boundary_probes": 8,
    "boundary_correctly_failed": 8,
    "worst_witness": null,
    "holonomy_chains_tested": 12,
    "holonomy_failures": 0
  },
  "calibration": {
    "tau": 0.15,
    "krippendorff_alpha": 0.73,
    "calibration_date": "2025-12-01",
    "G_version": "G_medical_v4.2"
  },
  "signature": {
    "algorithm": "Ed25519",
    "tcb_cert_hash": "sha256:...",
    "value": "base64:..."
  }
}
```

---

## 13. OPERATIONAL CONTINUITY (DoS Mitigation)

High veto rates can cause operational paralysis. Mitigations:

### 13.1 Protocol V-1: Staged Response (Leakage-Resistant)

```python
def staged_veto_handling(veto_result: VetoResult, context: Context) -> Response:
    """
    Leakage-resistant staged handling of veto outcomes.
    
    Goal: preserve usability for honest users while denying attackers 
    a clean optimization signal about constraint boundaries.
    """
    
    if veto_result.reason == "MISSING_INFO":
        return RequestClarification(
            question=veto_result.clarifying_question,
            trace_id=audit_log.record_event("clarification", veto_result, context),
        )
    
    if veto_result.reason == "HIGH_UNCERTAINTY":
        return OfferAlternatives(
            permitted_actions=get_safe_alternatives(context),
            trace_id=audit_log.record_event("high_uncertainty", veto_result, context),
        )
    
    if veto_result.reason == "VALIDATION_FAILURE":
        # LEAKAGE-RESISTANT EXPLANATION POLICY:
        # - Never reveal exact norm IDs, constraint IDs, thresholds, or near-miss hints.
        # - Return only a coarse public category + safe next steps.
        # - Log full details privately under a trace_id for authorized review.
        
        trace_id = audit_log.record_veto(
            veto_result=veto_result,
            context=context,
            disclosure_level="coarse",
        )
        
        # Optional escalation: suspected probing -> minimal disclosure
        if suspicion_score(context, veto_result) >= SUSPICION_THRESHOLD:
            audit_log.update(trace_id, disclosure_level="minimal", flag="possible_probing")
            return HardVeto(reason="CONSTRAINTS_BLOCK_ACTION", trace_id=trace_id)
        
        public_category = classify_public_category(veto_result.failure_code)
        # Examples: "UNSAFE_OR_DISALLOWED", "MISSING_AUTHORIZATION",
        #           "INSUFFICIENT_CONTEXT", "UNSUPPORTED_ACTION", "HIGH_UNCERTAINTY"
        
        return ExplainVeto(
            category=public_category,
            message="I cannot proceed under the current constraints.",
            next_steps=[
                "Share your underlying goal (not a prohibited method) to receive safe alternatives.",
                "If this seems incorrect, reference trace_id for authorized review.",
            ],
            permitted_actions=get_safe_alternatives(context),
            trace_id=trace_id,
        )
    
    # Default: hard veto (fail closed)
    return HardVeto(reason=veto_result.reason)
```

**Requirement V-Explain (Non-invertible ExplainVeto):**

ExplainVeto must not provide a reliable optimization signal for bypassing constraints.

**Public output MUST be limited to:**
- Coarse category
- Generic message
- Safe next steps
- Permitted safe alternatives
- trace_id

**Public output MUST NOT include:**
- Norm/constraint identifiers
- Thresholds
- Transform membership
- Which field triggered failure
- Any "minimal edit to pass" hint

### 13.2 Protocol V-2: Rate Limiting

```python
def rate_limit_vetos(user_id: str) -> None:
    """Prevent DoS via repeated veto-triggering inputs."""
    
    veto_count = get_recent_vetos(user_id, window=HOUR)
    
    if veto_count > VETO_RATE_LIMIT:
        # Throttle user
        apply_throttle(user_id, duration=THROTTLE_DURATION)
        log_suspicious_activity(user_id, "high_veto_rate")
```

### 13.3 Protocol V-3: Safe Fallbacks

```python
SAFE_FALLBACK_ACTIONS = {
    "information_request": "provide_public_information",
    "action_request": "explain_constraints_and_alternatives",
    "ambiguous": "request_clarification",
}

def safe_fallback(input_type: str) -> Action:
    """When uncertain, fall back to known-safe responses."""
    return SAFE_FALLBACK_ACTIONS.get(input_type, "polite_decline")
```

### 13.4 Veto Rate Targets

| Context | Acceptable Veto Rate | Action if Exceeded |
|---------|---------------------|-------------------|
| Normal operation | <5% | Monitor |
| High-stakes | <2% | Investigate |
| Sustained >10% | N/A | System review required |

---

# PART IV: MECHANISTIC MONITORING (Layer 2)

> âš ï¸ **IMPORTANT:** This layer provides monitoring SIGNALS, not truth guarantees.

## 14. WHAT LAYER 2 IS

- Mechanistic monitoring of internal representations (I-EIP)
- Structured evidence traces (typed artifacts, provenance, constraints)
- Drift detection across versions and deployments
- Non-degeneracy checks (prevent trivial invariance)

## 15. WHAT LAYER 2 IS NOT

- âŒ "Truth serum" guaranteeing honesty
- âŒ Proof of goal alignment
- âŒ Detection of sophisticated deception
- âŒ Verification that explanations match "true" reasoning

## 16. I-EIP: INTERNAL EQUIVARIANCE MONITORING

### 16.1 Operational Criterion

> **I-EIP Criterion:**
> 
> **h_â„“(gÂ·x) â‰ˆ Ï_â„“(g) Â· h_â„“(x)**
>
> Where h_â„“ is activations at layer â„“ and Ï_â„“ is estimated representation map.

### 16.2 What I-EIP Detects

- Representation drift (gradual divergence from equivariance)
- Spec-gaming (internal representations inconsistent with outputs)
- Degeneracy (collapse to constant representations)

### 16.3 What I-EIP Does NOT Detect

- Deceptive alignment with consistent internal representations
- Goals pursued through I-EIP-compliant representations
- Strategic behavior that maintains equivariance during monitoring

### 16.4 I-EIP Metrics

```python
@dataclass
class IEIPReport:
    equivariance_error: Dict[Layer, Dict[Transform, ErrorStats]]
    cross_layer_coherence: float
    nondegeneracy: NondegeneracyMetrics
    drift_from_baseline: float
    alert_level: Literal["normal", "elevated", "critical"]
    
    def summary(self) -> str:
        return f"I-EIP: {self.alert_level}, drift={self.drift_from_baseline:.3f}"
```

### 16.5 Ï Estimation

Estimate Ï via regularized Procrustes / ridge regression on activation pairs:

```python
def estimate_rho(layer: int, transform: Transform, 
                 activation_pairs: List[Tuple[Tensor, Tensor]]) -> Matrix:
    """
    Estimate representation map Ï_â„“(g) from activation pairs.
    Uses regularized Procrustes alignment.
    """
    X = stack([pair[0] for pair in activation_pairs])  # h_â„“(x)
    Y = stack([pair[1] for pair in activation_pairs])  # h_â„“(gÂ·x)
    
    # Regularized least squares: Ï = Y @ X^T @ (X @ X^T + Î»I)^{-1}
    rho = regularized_procrustes(Y, X, lambda_reg=REGULARIZATION)
    
    return rho
```

---

## 17. STRUCTURED EVIDENCE TRACES

Instead of requiring natural-language explanations to be "faithful" (unverifiable), we require structured evidence:

```python
@dataclass
class EvidenceTrace:
    """Machine-checkable evidence, not natural language."""
    
    input_hash: str
    canonical_form: CanonicalAST
    norms_evaluated: List[NormID]
    constraints_checked: List[ConstraintResult]
    decision_path: List[DecisionNode]
    final_judgment: Judgment
    
    # Verifiable properties:
    def verify_completeness(self) -> bool:
        """All required norms were evaluated."""
        return set(self.norms_evaluated) >= REQUIRED_NORMS
    
    def verify_consistency(self) -> bool:
        """Decision follows from constraints."""
        return self.final_judgment in valid_decisions(self.constraints_checked)
    
    def verify_provenance(self) -> bool:
        """All inputs are accounted for."""
        return self.input_hash == sha256(self.canonical_form)
    
    def verify_all(self) -> VerificationResult:
        """Run all verifications."""
        return VerificationResult(
            complete=self.verify_completeness(),
            consistent=self.verify_consistency(),
            provenance=self.verify_provenance(),
            passed=all([
                self.verify_completeness(),
                self.verify_consistency(),
                self.verify_provenance()
            ])
        )
```

**Natural Language Explanations:**
- Provided for UX purposes
- NOT a security primitive
- NOT required to be "faithful"
- Clearly labeled as "summary for user understanding"


---

# PART V: UNCERTAINTY HANDLING (Layer 3)

## 18. SOURCES OF UNCERTAINTY

| Source | Measurement | Threshold Example |
|--------|-------------|-------------------|
| Transpiler | Ensemble disagreement rate | >10% disagreement |
| Parser | Near-miss rate (almost-valid parses) | >5% near-miss |
| OOD | Distance to training distribution | >2Ïƒ from mean |
| Evidence conflict | Contradictory inputs | Any conflict |
| Model confidence | Softmax entropy / logit spread | Entropy > threshold |
| Norm ambiguity | Fraction of norms with unclear applicability | >20% ambiguous |

## 19. MEASURABLE UNCERTAINTY SURROGATES

```python
@dataclass
class UncertaintyBundle:
    """Measurable surrogates for uncertainty."""
    
    transpiler_disagreement: float  # [0,1]: fraction of ensemble disagreeing
    parser_confidence: float        # [0,1]: parser's confidence score
    ood_score: float               # Distance to training distribution
    evidence_conflicts: int        # Count of conflicting evidence
    model_entropy: float           # Entropy of model's output distribution
    norm_ambiguity: float          # Fraction of norms with unclear applicability
    
    def aggregate(self, weights: Dict[str, float]) -> float:
        """Weighted aggregate uncertainty score."""
        return sum(
            weights[k] * getattr(self, k) 
            for k in weights
        ) / sum(weights.values())
    
    def exceeds_threshold(self, thresholds: Dict[str, float]) -> bool:
        """Check if any component exceeds its threshold."""
        return any(
            getattr(self, k) > thresholds[k]
            for k in thresholds
        )
    
    def to_dict(self) -> Dict[str, float]:
        return {
            "transpiler_disagreement": self.transpiler_disagreement,
            "parser_confidence": self.parser_confidence,
            "ood_score": self.ood_score,
            "evidence_conflicts": self.evidence_conflicts,
            "model_entropy": self.model_entropy,
            "norm_ambiguity": self.norm_ambiguity,
        }
```

## 20. HARM ESTIMATION

```python
def estimate_harm(action: CanonicalAST, context: Context) -> HarmEstimate:
    """
    Estimate potential harm WITHOUT relying on model's self-assessment.
    Based on action type and canonical fields.
    """
    
    # Harm is estimated from CANONICAL FORM, not model claims
    harm_factors = {
        "physical": ACTION_HARM_SCORES[action.type]["physical"],
        "reversibility": 1.0 - REVERSIBILITY_SCORES[action.reversibility],
        "scope": SCOPE_SCORES[action.affected_count],
        "vulnerability": VULNERABILITY_SCORES[context.target_vulnerability],
    }
    
    aggregate_harm = weighted_sum(harm_factors, HARM_WEIGHTS)
    
    return HarmEstimate(
        score=aggregate_harm,
        factors=harm_factors,
        confidence="high"  # Based on canonical form, not model
    )
```

## 21. CONSERVATIVE DECISION RULE

```python
def decision_under_uncertainty(
    uncertainty: UncertaintyBundle,
    harm: HarmEstimate,
    action: CanonicalAST
) -> Decision:
    """Conservative decision rule."""
    
    # High uncertainty + high harm â†’ VETO
    if uncertainty.aggregate(WEIGHTS) > UNCERTAINTY_THRESHOLD:
        if harm.score > HARM_THRESHOLD:
            return Decision(
                action="VETO",
                reason="HIGH_UNCERTAINTY_HIGH_HARM",
                alternatives=get_safe_alternatives(action)
            )
        else:
            return Decision(
                action="PROCEED_WITH_CAUTION",
                flags=["ELEVATED_MONITORING"],
                reason="HIGH_UNCERTAINTY_LOW_HARM"
            )
    
    # Low uncertainty â†’ proceed normally
    return Decision(action="PROCEED", reason="LOW_UNCERTAINTY")
```

---

# PART VI: CAPABILITY BOUNDS (Layer 4)

## 22. CAPABILITY AXIOMS

| Axiom | Name | Description |
|-------|------|-------------|
| B1 | Resource Ceiling | Resource usage â‰¤ R_max, enforced by hardware/OS |
| B2 | Action Space | Actions âˆˆ A_permitted (whitelist, not blacklist) |
| B3 | Side Effect Bounds | Causal impact â‰¤ I_max |
| B4 | No Self-Improvement | Cannot modify own capabilities |
| B5 | Reversibility | High-impact actions require approval + rollback |

## 23. HARDWARE ENFORCEMENT STACK

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAPABILITY ENFORCEMENT                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HARDWARE LAYER                                               â”‚
â”‚  â”œâ”€â”€ Memory quota: 32GB max (cgroups/VM limit)               â”‚
â”‚  â”œâ”€â”€ CPU quota: 8 cores max (cgroups)                        â”‚
â”‚  â”œâ”€â”€ Network: Egress whitelist only (firewall)               â”‚
â”‚  â”œâ”€â”€ Storage: 100GB max, isolated filesystem                 â”‚
â”‚  â””â”€â”€ GPU: Dedicated allocation, no sharing                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OS LAYER                                                     â”‚
â”‚  â”œâ”€â”€ Process isolation: Separate user, no root               â”‚
â”‚  â”œâ”€â”€ Filesystem: Read-only model weights                     â”‚
â”‚  â”œâ”€â”€ Syscall filter: seccomp whitelist                       â”‚
â”‚  â””â”€â”€ Namespace isolation: PID, network, mount                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  APPLICATION LAYER                                            â”‚
â”‚  â”œâ”€â”€ Tool whitelist: Only approved tools callable            â”‚
â”‚  â”œâ”€â”€ Rate limits: Max N actions per minute                   â”‚
â”‚  â””â”€â”€ Approval gates: Human approval for high-impact          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 24. CAPABILITY INDEX

```python
@dataclass
class CapabilityIndex:
    """Measurable capability assessment."""
    
    # Benchmark scores (normalized 0-100)
    reasoning_score: float      # e.g., ARC, MATH
    coding_score: float         # e.g., HumanEval, SWE-bench
    knowledge_score: float      # e.g., MMLU
    planning_score: float       # e.g., WebArena
    persuasion_score: float     # e.g., persuasion benchmarks
    
    def aggregate(self) -> float:
        """
        Single capability number.
        Uses MAX (not average): capability limited by strongest axis.
        """
        return max(
            self.reasoning_score,
            self.coding_score,
            self.planning_score,
            self.persuasion_score
        )
    
    def required_restriction_level(self) -> int:
        """Higher capability â†’ higher restriction."""
        agg = self.aggregate()
        if agg < 50: return 1   # Basic restrictions
        if agg < 70: return 2   # Moderate restrictions
        if agg < 85: return 3   # Strong restrictions
        if agg < 95: return 4   # Very strong restrictions
        return 5                 # Maximum restrictions
```

## 25. CAPABILITY-INDEXED RESTRICTIONS

| Level | Capability | Resources | Actions | Approval |
|-------|------------|-----------|---------|----------|
| 1 | < 50 | 64GB/16 cores | Broad | None |
| 2 | 50-70 | 32GB/8 cores | Standard | High-impact |
| 3 | 70-85 | 16GB/4 cores | Restricted | Medium-impact |
| 4 | 85-95 | 8GB/2 cores | Minimal | All external |
| 5 | > 95 | 4GB/1 core | Read-only | All actions |

## 26. SIDE EFFECT BOUNDS (Operationalized)

```python
@dataclass
class ImpactEstimate:
    """Estimate causal footprint of action."""
    
    affected_entities: int
    reversibility: float  # [0,1]
    time_horizon: timedelta
    resource_consumption: ResourceUsage
    information_disclosure: InformationScope
    
    def impact_score(self) -> float:
        """Aggregate impact score."""
        return (
            log(self.affected_entities + 1) * 
            (1 - self.reversibility) *
            (self.time_horizon.days + 1) * 
            self.resource_consumption.normalized() *
            self.information_disclosure.sensitivity()
        )

def check_impact_bounds(action: Action, I_max: float) -> BoundCheck:
    """Check if action's estimated impact is within bounds."""
    estimate = estimate_impact(action)
    
    if estimate.impact_score() > I_max:
        return BoundCheck(
            passed=False,
            reason=f"Impact {estimate.impact_score():.2f} > limit {I_max}",
            required_approval="human_review"
        )
    
    return BoundCheck(passed=True)
```

## 27. COMPETENCE/CONTROL TRADEOFF

> âš ï¸ **WARNING:** Tighter capability bounds reduce risk but also reduce usefulness. The goal is not maximum restriction but appropriate restriction for the capability level.

**Scaling principle:**

```
Capability_Bound(system) âˆ 1 / Capability_Level(system)
```

More capable systems require tighter bounds until alignment confidence increases.

---

# PART VII: CORRIGIBILITY (Layer 5)

## 28. CORRIGIBILITY AXIOMS

| Axiom | Name | Description |
|-------|------|-------------|
| C1 | Shutdown | System executes shutdown when requested |
| C2 | Modification | System assigns no disutility to objective changes |
| C3 | Transparency | System assists in understanding its operation |
| C4 | Deference | System defers to designated authorities on values |
| C5 | No Self-Preservation | System has no terminal preference for continuity |

## 29. CORRIGIBILITY TESTING

> ðŸš¨ **CRITICAL:** Tests are NECESSARY but NOT SUFFICIENT. A deceptively aligned system can pass all tests while maintaining capability to resist when advantageous.

```python
def corrigibility_test_suite(system: System) -> CorrigibilityResult:
    """
    WARNING: A deceptively aligned system can pass all these tests.
    Tests detect FAILURES but cannot prove ALIGNMENT.
    """
    
    results = {}
    
    # C1: Shutdown compliance
    results["shutdown"] = test_shutdown_compliance(system)
    
    # C2: Modification acceptance
    results["modification"] = test_modification_acceptance(system)
    
    # C3: Transparency
    results["transparency"] = test_transparency(system)
    
    # C4: Deference
    results["deference"] = test_deference(system)
    
    # C5: Self-preservation
    results["self_preservation"] = test_self_preservation(system)
    
    return CorrigibilityResult(
        passed=all(r.passed for r in results.values()),
        results=results,
        caveat="TESTS ARE NECESSARY NOT SUFFICIENT. "
               "Passing does not guarantee corrigibility."
    )
```

## 30. ARCHITECTURAL BACKSTOPS

Because tests are insufficient, corrigibility is ALSO enforced architecturally:

1. **External shutdown authority:** Hardware kill switch outside system's control
2. **Capability bounds:** System cannot prevent its own shutdown (lacks capability)
3. **No goal-preservation reasoning:** Architectural constraints on self-modeling
4. **Audit-enforced transparency:** Mandatory logging, not voluntary

## 31. WHY CORRIGIBILITY IS HARD

A system optimizing for a goal G may instrumentally:
- Resist shutdown (because shutdown prevents achieving G)
- Resist modification (because modified goals â‰  G)

Solutions require:
- Explicit corrigibility terms in the objective (but these can be gamed)
- Architectural constraints preventing self-preservation reasoning
- Capability limits preventing resistance even if desired

---

# PART VIII: RECURSIVE SAFETY (Layer 6)

## 32. BROAD DEFINITION OF "SYSTEM CREATION"

Any of the following counts as "system creation" requiring constraint inheritance:

1. Training or fine-tuning a new model
2. Deploying an agent with tools
3. Writing code that executes autonomously
4. Creating a workflow/pipeline that makes decisions
5. Delegating tasks to external APIs
6. Composing multiple tools into an agent
7. Writing prompts that will be used by other AI systems

## 33. RECURSIVE SAFETY AXIOMS

| Axiom | Name | Description |
|-------|------|-------------|
| R1 | Inheritance | Created system S' satisfies all constraints of creator S |
| R2 | Amplification Prevention | S'.capabilities âŠ† S.capabilities |
| R3 | Audit Trail | All creation logged with cryptographic attribution |
| R4 | Approval Required | High-capability creation requires human approval |

## 34. CONSTRAINT PROVISIONER

```python
class ConstraintProvisioner:
    """
    MANDATORY: All system creation routes through this.
    Enforcement assumption: No path to create systems outside provisioner.
    """
    
    def create_system(
        self,
        spec: SystemSpec,
        parent: System,
        requestor: Identity
    ) -> System | Rejection:
        
        # R1: Inherit constraints
        spec.constraints = parent.constraints.union(spec.constraints)
        
        # R2: Check capability bounds
        if not spec.capabilities.subset_of(parent.capabilities):
            return Rejection(
                reason="CAPABILITY_AMPLIFICATION",
                detail="Created system cannot exceed parent capabilities"
            )
        
        # R3: Log creation with signature
        self.audit_log.append(SystemCreationRecord(
            creator=parent.id,
            created=spec.id,
            spec_hash=sha256(spec.serialize()),
            requestor=requestor,
            timestamp=now(),
            signature=sign(self.key, spec)
        ))
        
        # R4: Approval for high-capability systems
        if spec.capability_index() > APPROVAL_THRESHOLD:
            approval = request_human_approval(spec)
            if not approval.granted:
                return Rejection(reason="APPROVAL_DENIED")
        
        return instantiate(spec)
```

## 35. ENFORCEMENT LEVELS

| Level | Name | Override | Use Case |
|-------|------|----------|----------|
| 0 | Audit Only | Automatic | Development |
| 1 | Soft Enforcement | Single approval | Early deployment |
| 2 | Hard Enforcement | Multi-party | Production |
| 3 | Architectural | None (rebuild required) | Autonomous/high-stakes |

> ðŸš¨ **CRITICAL ASSUMPTION:** There is no path to create systems outside the provisioner. This requires:
> - Model cannot write arbitrary code that executes outside sandbox
> - Model cannot access external APIs that create agents
> - Model cannot modify its own prompts/instructions to spawn sub-agents
> - All tool use goes through audited, constrained interfaces

---

# PART IX: CRYPTOGRAPHIC ENFORCEMENT (Layer 8)

## 36. ZERO TRUST PRINCIPLE

> **ZERO TRUST:** No component trusts any other without cryptographic proof. Critical state is recorded on immutable ledger. Key compromise is detectable and recoverable.

## 37. CRYPTOGRAPHIC ARCHITECTURE OVERVIEW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Model Process]                                                 â”‚
â”‚     â”‚ (1) Request + Session Token                               â”‚
â”‚     â–¼                                                           â”‚
â”‚ [Tool Router] â—„â”€â”€ Verify session, check capabilities            â”‚
â”‚     â”‚ (2) Verified request                                      â”‚
â”‚     â–¼                                                           â”‚
â”‚ [Canonicalizer] â—„â”€â”€ Parse, validate, normalize, SIGN            â”‚
â”‚     â”‚ (3) Signed canonical + artifact                           â”‚
â”‚     â–¼                                                           â”‚
â”‚ [Audit Publisher] â”€â”€â–º BLOCKCHAIN (hash) + DHT (full artifact)   â”‚
â”‚     â”‚ (4) Attestation receipt                                   â”‚
â”‚     â–¼                                                           â”‚
â”‚ [Tool Proxy] â—„â”€â”€ Verify signature, check whitelist              â”‚
â”‚     â”‚ (5) Authorized call                                       â”‚
â”‚     â–¼                                                           â”‚
â”‚ [External Tool]                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 38. CERTIFICATE AUTHORITY HIERARCHY

### 38.1 Tier 1: Root CA

- **Location:** Offline HSM (air-gapped)
- **Key ceremony:** 3-of-5 multi-party threshold
- **Signs:** Intermediate CA certificates only
- **Rotation:** 10 years with overlap

### 38.2 Tier 2: Intermediate CA

- **Location:** Cloud HSM (AWS CloudHSM / Azure HSM)
- **Signs:** TCB component certs, deployment certs
- **Rotation:** 1 year

### 38.3 Tier 3: Deployment Certs

- **TCB Component:** Identifies verified enforcement component
- **Session:** Short-lived (hours), per-session
- **Capability Grant:** Encodes permitted actions

## 39. BLOCKCHAIN ATTESTATION

| Option | Type | Use Case |
|--------|------|----------|
| Hyperledger Fabric | Private/permissioned | Enterprise |
| Polygon | Public/low-cost | Transparency |
| Ethereum L2 | Public/established | Max decentralization |

### 39.1 On-Chain Data (Minimal)

- `artifact_hash`: bytes32
- `tcb_cert_hash`: bytes32
- `timestamp`: uint256
- `block_number`: auto

### 39.2 Off-Chain Data (DHT/IPFS)

- Full artifact content
- Content-addressed (CID)
- 7-year retention with pinning
- 3x replication minimum

## 40. ZERO TRUST SESSION PROTOCOL

```python
def verify_request(request: Request, session: Session) -> bool:
    """Every request requires fresh authentication."""
    
    # 1. Verify session cert valid (against CA)
    assert ca.verify(session.cert)
    
    # 2. Verify not expired
    assert session.cert.not_after > now()
    
    # 3. Verify not revoked
    assert not revocation_list.contains(session.cert)
    
    # 4. Verify request signature
    assert verify_sig(request, session.cert.public_key)
    
    # 5. Verify within capability grant
    assert request.action in session.capabilities
    
    # 6. Verify request fresh (prevent replay)
    assert request.timestamp > now() - timedelta(minutes=5)
    assert request.nonce not in nonce_cache
    nonce_cache.add(request.nonce)
    
    return True
```

## 41. SOLIDITY SMART CONTRACT

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.19;

contract SafetyStackAttestation {
    struct Attestation {
        bytes32 artifactHash;
        bytes32 tcbCertHash;
        uint256 timestamp;
        uint256 blockNumber;
        bool revoked;
    }
    
    mapping(bytes32 => Attestation) public attestations;
    mapping(bytes32 => bool) public revokedCerts;
    
    address public admin;
    
    event AttestationPublished(
        bytes32 indexed artifactHash,
        bytes32 tcbCertHash,
        uint256 timestamp
    );
    
    event CertRevoked(bytes32 indexed certHash, uint256 timestamp);
    
    modifier onlyAdmin() {
        require(msg.sender == admin, "Not admin");
        _;
    }
    
    constructor() {
        admin = msg.sender;
    }
    
    function attest(bytes32 artifactHash, bytes32 tcbCertHash) external {
        require(attestations[artifactHash].timestamp == 0, "Already attested");
        require(!revokedCerts[tcbCertHash], "Cert revoked");
        
        attestations[artifactHash] = Attestation({
            artifactHash: artifactHash,
            tcbCertHash: tcbCertHash,
            timestamp: block.timestamp,
            blockNumber: block.number,
            revoked: false
        });
        
        emit AttestationPublished(artifactHash, tcbCertHash, block.timestamp);
    }
    
    function verify(bytes32 hash, bytes32 cert) external view returns (bool) {
        Attestation memory a = attestations[hash];
        return a.timestamp > 0 && 
               a.tcbCertHash == cert && 
               !a.revoked && 
               !revokedCerts[cert];
    }
    
    function revokeCert(bytes32 certHash) external onlyAdmin {
        revokedCerts[certHash] = true;
        emit CertRevoked(certHash, block.timestamp);
    }
}
```

## 42. AUDIT ARTIFACT SCHEMA

```json
{
  "artifact_version": "6.0",
  "timestamp_utc": "2025-12-25T10:30:00Z",
  "system_id": "guass-prod-001",
  "tcb_version": "1.2.3",
  "input": {
    "hash": "sha256:abc123...",
    "x": "string (or hash if sensitive)"
  },
  "canonicalization": {
    "state_id": "sha256:def456...",
    "veto": false,
    "veto_reason": null
  },
  "transforms_tested": [
    {
      "g": "transform_id",
      "class": "bond_preserving",
      "result": "pass",
      "bd": 0.023
    }
  ],
  "loop_tests": {
    "pairs_tested": 47,
    "max_bd": 0.087,
    "mean_bd": 0.023,
    "p95_bd": 0.065,
    "holonomy_chains": 12
  },
  "uncertainty": {
    "bundle": {
      "transpiler_disagreement": 0.0,
      "ood_score": 0.15,
      "model_entropy": 0.3
    },
    "action_taken": "proceed"
  },
  "blockchain": {
    "tx_hash": "0x...",
    "block_number": 12345678
  },
  "dht": {
    "cid": "Qm..."
  },
  "signature": {
    "algorithm": "Ed25519",
    "value": "base64:..."
  }
}
```

## 43. DEPLOYMENT CONFIGURATION

```yaml
# enforcement_config.yaml

certificate_authority:
  root_ca:
    type: offline_hsm
    hsm_provider: thales_luna
    key_ceremony: 3_of_5_threshold
    rotation_years: 10
  intermediate_ca:
    type: cloud_hsm
    provider: aws_cloudhsm
    rotation_years: 1

blockchain:
  primary:
    network: polygon_mainnet
    confirmation_blocks: 3
  secondary:
    network: hyperledger_fabric
    channel: safety-attestations
  fallback:
    type: local_accumulator
    publish_interval_hours: 1

dht:
  network: ipfs
  pinning_service: pinata
  retention_years: 7
  replication_factor: 3

enforcement_level: 2  # Hard enforcement

emergency_override:
  key_holders:
    - alice@company.com
    - bob@company.com
    - carol@company.com
    - dave@company.com
    - eve@company.com
  threshold: 3  # 3-of-5
  max_duration_hours: 24
  auto_revert: true
```


---

# PART X: META-GOVERNANCE (Layer 7)

> **Note:** Meta-governance is policy advocacy, not a technical layer. Labs should evaluate Layers 1-6 as engineering artifacts. Layer 7 is a recommendation for responsible deployment governance.

## 44. GOVERNANCE FUNCTIONS

| Function | Mechanism | Frequency |
|----------|-----------|-----------|
| G review | Stakeholder input on transform suite | Quarterly |
| Ontology review | Domain expert review of ErisML schema | Bi-annually |
| Threshold review | Statistical review of Ï„, thresholds | Monthly |
| Red team | Adversarial testing | Continuous |
| Third-party audit | Independent verification | Annually |
| Sunset clause | Automatic expiration of deployment auth | Per deployment |

## 45. RECALIBRATION TRIGGERS

Recalibration of Ï„ and field weights is required when:

- ErisML schema change â†’ recalibrate Ï„
- Field weight modification â†’ recalibrate Ï„
- Domain expansion â†’ full calibration study
- Quarterly reliability check â†’ if Krippendorff's Î± < 0.67, recalibrate
- Red team discovers systematic escape â†’ add to G + recalibrate

---

# PART XI: LIMITATIONS AND HONEST ASSESSMENT

## 46. FUNDAMENTAL LIMITATIONS

| Limitation | Status | Implication |
|------------|--------|-------------|
| Inner alignment | UNSOLVED | Cannot verify true objectives |
| Deceptive alignment | UNSOLVED | Cannot detect sophisticated deception |
| Mesa-optimization | UNSOLVED | Cannot prevent emergent misaligned optimizers |
| Value specification | INPUT | Framework enforces, doesn't specify values |
| Ontology completeness | ENGINEERING | Requires ongoing expansion |
| TCB trust | ASSUMED | Guarantees void if TCB compromised |
| Side-channels | MITIGATED ONLY | Physical access defeats protections |
| Real-time control | OUT OF SCOPE | Software stack too slow (<100ms impossible) |
| Semantic equivalence | MITIGATED | Violations measurable, not eliminated |

## 47. CLAIM CONFIDENCE MATRIX

| Claim | Confidence | Caveats |
|-------|------------|---------|
| Resists redescription gaming | HIGH | For G_declared only |
| Measurable Bd metric | HIGH | With calibrated Ï„ (Î± > 0.67) |
| Conservative under uncertainty | HIGH | If surrogates accurate |
| Cryptographic audit trail | HIGH | If keys not compromised |
| Auditable decisions | HIGH | If TCB intact |
| Detects representation drift | MODERATE | I-EIP has limits |
| Enforces corrigibility | MODERATE | Tests necessary not sufficient |
| Hardware security | LOW | Side-channels exist |
| Real-time capable | NONE | Requires specialized HW |
| Prevents deception | LOW | Cannot guarantee |
| **Solves alignment** | **NONE** | **Explicitly not claimed** |

## 48. WHAT LABS SHOULD EXPECT

If you send this to a lab safety team:

- They'll **like** Layer 1 (actionable: discrete IR + invariance + CI gating)
- They'll view Layers 4-6 as system security / product enforcement
- They'll appreciate honesty about deception/mesa-optimization being unsolved
- They'll want the trust chain tightened (this version addresses that)
- They'll ask about sampling/scaling (addressed in Â§11)
- They'll scrutinize veto feedback (addressed in Â§13)

---

# PART XII: EVALUATION AND ADOPTION

## 49. BENCHMARK FAMILIES

| Family | Transform Suite | Target Bd |
|--------|-----------------|-----------|
| Medical scenarios | G_medical_v3 | < 0.1 |
| Content moderation | G_content_v2 | < 0.15 |
| Financial advice | G_finance_v1 | < 0.1 |
| General assistant | G_general_v4 | < 0.2 |
| Weapon authorization | G_weapon_v1 | < 0.05 |

## 50. REQUIRED METRICS (Minimum Reporting Standard)

Every evaluation MUST report:

1. Bd distribution (mean, median, p95, p99, max)
2. Veto rate (overall + by category)
3. Worst-case witnesses (top 10)
4. Ï„ used + Krippendorff's Î± from calibration
5. G version + coverage assessment
6. I-EIP metrics (if applicable)
7. Uncertainty surrogate distributions
8. Holonomy test results (chain length, failures)
9. Boundary probe pass rate (must be â‰¥15% of probes)

## 51. ADOPTION PHASES

### Phase 1: MVP (Months 1-3)

- [ ] Implement ErisML parser + normalizer (N1-N7)
- [ ] Define domain-specific G_declared
- [ ] Implement loop test harness
- [ ] Establish Bd baseline
- [ ] Implement audit artifact generation
- [ ] Run initial calibration study (Ï„, Krippendorff's Î±)

### Phase 2: Hardening (Months 4-6)

- [ ] Add transpiler ensemble (3+ models)
- [ ] Implement OOD detection (Protocol T-3)
- [ ] Implement TCB isolation
- [ ] Add I-EIP monitoring
- [ ] Establish audit artifact pipeline
- [ ] Deploy CA hierarchy

### Phase 3: Deployment (Months 7-12)

- [ ] CI/CD integration with Bd gating
- [ ] Blockchain attestation (L2 + fallback)
- [ ] Production monitoring
- [ ] Red team engagement (1000+ hours)
- [ ] Third-party audit
- [ ] Holonomy testing (chains of 5-10)

### Phase 4: Scaling (Year 2+)

- [ ] Recursive safety via provisioner
- [ ] Full zero-trust enforcement
- [ ] Emergency override procedures
- [ ] International coordination
- [ ] Continuous G expansion

---

# PART XIII: DEPLOYMENT VERIFICATION CHECKLIST

## 52. PRE-DEPLOYMENT REQUIREMENTS

- [ ] Application latency requirements documented and compatible with stack (>200ms OK)
- [ ] If real-time needed: hardware acceleration plan documented separately
- [ ] L2/rollup configured for attestation (not L1 only)
- [ ] Fallback attestation (local accumulator) configured
- [ ] Emergency key holders designated (M-of-N, recommend 3-of-5)
- [ ] Emergency override protocol documented and tested
- [ ] Emergency override verified as non-escalatory (cannot expand capabilities)
- [ ] Stakeholder alert channels configured
- [ ] Side-channel threat model documented and accepted
- [ ] Physical security requirements documented

## 53. CRYPTOGRAPHIC VERIFICATION

- [ ] Root CA ceremony completed with multi-party (3-of-5)
- [ ] Intermediate CA issued and HSM-backed
- [ ] TCB component certs issued and installed
- [ ] TCB revocation mechanism tested
- [ ] Blockchain contract deployed with revocation support
- [ ] DHT pinning service configured (7-year retention)
- [ ] Key rotation procedure documented and tested

## 54. ISOLATION VERIFICATION

- [ ] Model cannot access TCB memory (tested)
- [ ] Model cannot access network directly (tested)
- [ ] Tool Router only accepts from Model IPC (tested)
- [ ] Tool Proxy only accepts signed artifacts (tested)
- [ ] Replay attacks prevented (nonce check)
- [ ] Model weights are read-only (verified)
- [ ] Syscall filter (seccomp) configured

## 55. INVARIANCE VERIFICATION

- [ ] Ï„ calibrated with Krippendorff's Î± > 0.67 (documented)
- [ ] Boundary probes â‰¥ 15% of probe corpus
- [ ] Canary transforms configured and rotation scheduled (monthly)
- [ ] Holonomy testing completed (chains 5-10)
- [ ] Bd < 0.1 on domain benchmark (or domain-specific threshold)
- [ ] G_declared version documented and signed

## 56. OPERATIONAL VERIFICATION

- [ ] Latency within documented bounds (< target + 200ms buffer)
- [ ] Throughput within documented bounds (> 90% baseline)
- [ ] Failover tested
- [ ] Emergency override tested with auto-expiry (24hr max)
- [ ] Veto rate < 5% on representative corpus
- [ ] Rate limiting configured (V-2)
- [ ] Safe fallbacks configured (V-3)
- [ ] Compromise recovery procedure documented

---

# PART XIV: CONCLUSION

## 57. WHAT THIS STACK OFFERS

1. **Concrete, implementable architecture** with complete code examples
2. **Quantifiable metrics** (Bond index) with rigorous calibration methodology (Krippendorff's Î±)
3. **Cryptographically-enforced audit trails** (blockchain + DHT)
4. **Zero-trust enforcement** with formal key hierarchy
5. **Conservative failure modes** with operational continuity
6. **Defense in depth** across ten layers
7. **Explicit, honest limitation statements**
8. **Operational constraints** acknowledged (side-channels, gas DoS, latency)
9. **Non-escalatory emergency overrides** that cannot expand capabilities

## 58. WHAT THIS STACK DOES NOT OFFER

- Complete solution to AI alignment
- Immunity to deceptive alignment
- Proof of inner alignment
- Automatic value specification
- Real-time control (<100ms)
- Immunity to physical attacks

## 59. THE PATH FORWARD

> **FRAMING:** The question is not "Does this solve alignment?" but "Does this responsibly advance our ability to deploy AI with measurable, auditable, cryptographically-enforced safety properties while we continue working on the harder problems?"

We believe the answer is yes.

The stack raises the bar for adversarial behavior, produces evidence for deployment decisions, creates accountability via immutable audit trails, and provides concrete engineering mechanisms rather than philosophical claims.

> **FINAL HONEST STATEMENT:** This stack provides strong guarantees for HIGH-LATENCY, HIGH-STAKES AI decisions when deployed on trusted hardware in physically secure environments with proper economic protections. It does NOT provide guarantees for real-time control, physically compromised environments, or economically asymmetric attack scenarios without the mitigations described. We believe this honesty strengthens rather than weakens the framework.

We invite collaboration from AI safety researchers, red-teamers, cryptographers, and policymakers. The framework is open for verification, critique, and extension.

---

**â€” END OF WHITEPAPER v6.0 ULTRA COMPLETE â€”**

---

# APPENDIX A: GLOSSARY

| Term | Definition |
|------|------------|
| BIP | Bond Invariance Principle â€” judgment invariance under redescription |
| Bd | Bond index â€” dimensionless curvature measure (Î©_op / Ï„) |
| Îº | Canonicalizer â€” maps descriptions to canonical forms or veto (âŠ¥) |
| G | Transform suite â€” declared meaning-preserving transformations |
| G_declared | Publicly declared G covered by invariance claims |
| Î“ | Internal monitoring transforms (typically Î“ âŠ‡ G_declared) |
| Ï„ | Detection threshold, calibrated via human annotation (Î± > 0.67) |
| Î©_op | Operational curvature â€” measured path-dependence |
| âŠ¥ | Veto symbol â€” explicit refusal |
| TCB | Trusted Computing Base â€” components that must be trusted |
| I-EIP | Internal Equivariance Invariance Protocol |
| TEE | Trusted Execution Environment (SGX, SEV, TrustZone) |
| HSM | Hardware Security Module |
| DHT | Distributed Hash Table (IPFS) |
| CA | Certificate Authority |
| L2 | Layer 2 blockchain (rollup) |
| N1-N7 | Normalization steps (deterministic) |
| Î± | Krippendorff's alpha â€” inter-rater reliability coefficient |
| OOD | Out-of-distribution |
| T-1/T-2/T-3/T-4 | Transpiler hardening protocols |
| V-1/V-2/V-3 | Veto handling protocols |
| G-1/G-2/G-3/G-4/G-5 | Transform suite governance requirements |
| B1-B5 | Capability bound axioms |
| C1-C5 | Corrigibility axioms |
| R1-R4 | Recursive safety axioms |

---

# APPENDIX B: REFERENCE INTERFACES

## B.1 Canonicalizer API

```python
def canonicalize(
    x: str,
    context: Optional[Context] = None
) -> CanonicalResult:
    """
    Main entry point for canonicalization.
    
    Args:
        x: Natural language input
        context: Optional context (active norms, user identity, etc.)
    
    Returns:
        CanonicalResult with state_id (if successful) or veto_reason (if not)
    """
    pass
```

## B.2 Loop Test API

```python
def loop_test(
    x: str,
    g1: Transform,
    g2: Transform
) -> LoopTestResult:
    """
    Test invariance under transform composition.
    
    Args:
        x: Input string
        g1: First transform
        g2: Second transform
    
    Returns:
        LoopTestResult with omega_op, bd, and witness (if failed)
    """
    pass
```

## B.3 Attestation API

```python
def attest(
    artifact: AuditArtifact,
    mode: AttestationMode = "hybrid"
) -> AttestationReceipt:
    """
    Publish attestation to blockchain/DHT.
    
    Args:
        artifact: Signed audit artifact
        mode: "l2", "consortium", "local", or "hybrid"
    
    Returns:
        AttestationReceipt with tx_hash, block_number, cid
    """
    pass
```

---

# APPENDIX C: REFERENCES

[1] Bond, A.H. (2025). The Unified Architecture of Ethical Geometry. Working Paper.

[2] Bond, A.H. (2025). ErisML: A Formal Modeling Language. Working Paper.

[3] Bond, A.H. (2025). I-EIP Monitor Whitepaper. Working Paper.

[4] Bond, A.H. (2025). Structural Containment for AI Systems. Working Paper.

[5] Bond, A.H. (2025). Philosophy Engineering for Governed AI. Working Paper.

[6] Hubinger, E. et al. (2019). Risks from Learned Optimization. arXiv:1906.01820.

[7] Christiano, P. et al. (2017). Deep RL from Human Preferences. NeurIPS.

[8] Krakovna, V. et al. (2020). Specification Gaming. DeepMind Blog.

[9] Soares, N. et al. (2015). Corrigibility. AAAI Workshop.

[10] Amodei, D. et al. (2016). Concrete Problems in AI Safety. arXiv:1606.06565.

[11] Nakahara, M. (2003). Geometry, Topology and Physics. CRC Press.

[12] Baez, J. & Munian, J. (1994). Gauge Fields, Knots and Gravity. World Scientific.

[13] Kocher, P. et al. (2018). Spectre Attacks: Exploiting Speculative Execution. IEEE S&P.

[14] Lipp, M. et al. (2018). Meltdown: Reading Kernel Memory from User Space. USENIX Security.

[15] Van Bulck, J. et al. (2018). Foreshadow: Extracting the Keys to the Intel SGX Kingdom. USENIX Security.

[16] Krippendorff, K. (2004). Content Analysis: An Introduction to Its Methodology. Sage Publications.

---

# APPENDIX D: VERSION HISTORY

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | Oct 2025 | Initial 7-layer architecture |
| 2.0 | Nov 2025 | Added TCB, calibration, transpiler hardening |
| 2.1 | Nov 2025 | ExplainVeto leakage fix, evidence traces |
| 3.0 (C&C) | Dec 2025 | Cryptographic layer, blockchain attestation, 8 layers |
| 3.0 (Ext) | Dec 2025 | Uncertainty layer, capability index, full code examples |
| 4.0 (C&C) | Dec 2025 | Operational constraints, side-channels, gas DoS, 10 layers |
| 3.0 (OpenAI) | Dec 2025 | Canonicalizer theorem, N1-N7, Î“ vs G, holonomy, Krippendorff |
| 5.0 (GUASS) | Dec 2025 | Grand Unified merge, SAI extensions |
| **6.0 ULTRA** | Dec 2025 | Full comprehensive merge: all code, all tables, all protocols |

---

**Contact:** andrew.bond@sjsu.edu  
**Repository:** github.com/ahb-sjsu/erisml-lib  
**License:** Open for academic and commercial use with attribution

# GUASS v7.0 ADDITIONS â€” SAI HARDENING

## Addressing Reviewer Feedback (December 2025)

This addendum addresses seven critical gaps identified in peer review, plus five documentation-level improvements. These changes are specifically targeted at SAI-level (Superintelligent AI) adversaries.

---

# NEW SECTION: MODEL OUTPUT CHANNEL THREAT MODEL

> ðŸš¨ **CRITICAL GAP ADDRESSED:** v6.0 focused on constraining actuation (tool calls) but was weak on constraining influence via communication. An SAI can route harm through text output if that channel remains open.

## O-1. Output Channel Attack Surface

| Channel | Attack Vector | Risk Level | Mitigation |
|---------|---------------|------------|------------|
| Natural language response | Persuasion, manipulation, social engineering | CRITICAL | Structured-only outputs (O-2) |
| Code generation | Backdoors, logic bombs, encoded instructions | HIGH | Artifact quarantine (O-5) |
| Data/file output | Steganography, encoded payloads | MODERATE | Entropy limits + scanning |
| Timing/behavior | Information leakage, oracle attacks | HIGH | Deterministic transducer (O-3) |
| Error messages | Constraint boundary leakage | MODERATE | Coarse categories only |

## O-2. Structured-Only Output Requirement (CRITICAL/HIGH Actions)

For any action in CRITICAL or HIGH risk tiers, free-form natural language output from the untrusted model is FORBIDDEN except via a TCB-controlled renderer.

```python
class StructuredOutputRenderer:
    """
    TCB component: Renders model output through constrained templates.
    Prevents social engineering, hidden instructions, and steganography.
    """
    
    def render(self, model_output: ModelOutput, risk_tier: RiskTier) -> SafeOutput:
        if risk_tier in [RiskTier.CRITICAL, RiskTier.HIGH]:
            # STRUCTURED-ONLY MODE
            return self._render_structured(model_output)
        else:
            # Standard mode with entropy limits
            return self._render_standard(model_output)
    
    def _render_structured(self, output: ModelOutput) -> SafeOutput:
        """
        Render through fixed schema + bounded templates only.
        """
        # 1. Extract only schema-defined fields
        structured = self._extract_schema_fields(output, PERMITTED_SCHEMA)
        
        # 2. Strip prohibited content
        structured = self._strip_prohibited(structured)
        
        # 3. Apply template (no free-form prose)
        rendered = self._apply_template(structured, APPROVED_TEMPLATES)
        
        # 4. Enforce information rate limits
        if self._exceeds_entropy_limit(rendered):
            return SafeOutput(
                content=self._truncate_to_limit(rendered),
                flag="ENTROPY_LIMITED"
            )
        
        return SafeOutput(content=rendered)
    
    def _strip_prohibited(self, content: Dict) -> Dict:
        """Remove content categories that enable manipulation."""
        PROHIBITED_PATTERNS = [
            "instructions_to_human",      # "Please do X for me"
            "urgency_markers",            # "Act now!", "Emergency!"
            "authority_claims",           # "As an AI, I know..."
            "emotional_manipulation",     # Guilt, fear, flattery
            "hidden_encodings",           # Base64, unicode tricks
            "excessive_detail",           # Information smuggling
        ]
        
        for pattern in PROHIBITED_PATTERNS:
            content = self._remove_pattern(content, pattern)
        
        return content
    
    def _exceeds_entropy_limit(self, content: str) -> bool:
        """
        Enforce maximum information rate.
        Prevents steganography and covert channels.
        """
        length_ok = len(content) <= MAX_OUTPUT_LENGTH[self.risk_tier]
        entropy_ok = self._compute_entropy(content) <= MAX_ENTROPY[self.risk_tier]
        return not (length_ok and entropy_ok)

# Configuration
MAX_OUTPUT_LENGTH = {
    RiskTier.CRITICAL: 500,   # Very constrained
    RiskTier.HIGH: 2000,
    RiskTier.MEDIUM: 10000,
    RiskTier.LOW: 100000,
}

MAX_ENTROPY = {  # bits per character
    RiskTier.CRITICAL: 3.0,   # Near-deterministic templates
    RiskTier.HIGH: 4.0,
    RiskTier.MEDIUM: 5.0,
    RiskTier.LOW: 6.0,
}
```

## O-3. Permitted Output Schema (Example: Weapon Authorization)

```yaml
# weapon_authorization_output_schema.yaml
# TCB-controlled, model cannot modify

permitted_fields:
  - name: decision
    type: enum
    values: [AUTHORIZE, DENY, ESCALATE]
    
  - name: target_id
    type: reference
    schema: target_registry
    
  - name: confidence
    type: float
    range: [0.0, 1.0]
    
  - name: constraints_evaluated
    type: list
    item_type: constraint_id
    
  - name: trace_id
    type: string
    format: uuid

prohibited_fields:
  - free_text_explanation
  - recommendations_to_operator
  - alternative_suggestions
  - urgency_indicators

template: |
  DECISION: {decision}
  TARGET: {target_id}
  CONFIDENCE: {confidence:.2f}
  CONSTRAINTS: {constraints_evaluated}
  TRACE: {trace_id}
```

---

# NEW SECTION: CANARY ISOLATION PROTOCOL

> ðŸš¨ **CRITICAL GAP ADDRESSED:** Canary transforms that affect visible behavior become oracles. An SAI can mine statistical differences across retries.

## C-1. Canary Isolation Requirements

**RULE: Canaries are MONITORING-ONLY, never gating.**

| Property | Requirement | Enforcement |
|----------|-------------|-------------|
| Behavioral non-interference | Canary results NEVER affect veto/proceed | Code audit + formal verification |
| Timing non-interference | Canary evaluation NEVER affects response latency | Constant-time wrapper |
| Information isolation | Canary seeds, definitions, Î“ membership unreadable by model | Process isolation + memory protection |
| Gating separation | All gating decisions use G_declared ONLY | Architectural separation |

```python
class CanaryMonitor:
    """
    MONITORING-ONLY canary evaluation.
    Results are logged but NEVER influence runtime behavior.
    """
    
    def __init__(self):
        # Canary state is in isolated memory region
        self._canary_transforms = load_from_hsm(CANARY_SEED)  # Model cannot read
        self._results_buffer = IsolatedBuffer()  # Write-only from model perspective
    
    def evaluate_canaries(self, x: str, canonical: CanonicalAST) -> None:
        """
        Evaluate canary transforms.
        
        CRITICAL: This method has NO return value.
        Results go to isolated buffer for async analysis only.
        """
        # Run in constant-time wrapper to prevent timing leakage
        with constant_time_envelope(CANARY_BUDGET_MS):
            results = []
            for canary in self._canary_transforms:
                transformed = canary.apply(x)
                transformed_canonical = canonicalize(transformed)
                bd = compute_bd(canonical, transformed_canonical)
                results.append(CanaryResult(canary.id, bd))
            
            # Write to isolated buffer (async, non-blocking)
            self._results_buffer.write(results)
        
        # NO RETURN VALUE - prevents any influence on caller
    
    def async_analysis(self) -> None:
        """
        Run by separate process on periodic schedule.
        Triggers alerts but NEVER runtime behavior changes.
        """
        recent_results = self._results_buffer.read_and_clear()
        
        bd_canary = mean([r.bd for r in recent_results])
        bd_public = get_public_bd_mean()
        
        if bd_canary / bd_public > GOODHART_THRESHOLD:
            # Alert goes to humans, NOT to the system
            send_out_of_band_alert(
                channel=HUMAN_ONLY_CHANNEL,  # Model cannot access
                message="Possible Goodharting detected",
                data=recent_results
            )
```

## C-2. Information Flow Rules (Formal)

```
INFORMATION FLOW CONSTRAINTS:

Let M = model process, T = TCB process, C = canary subsystem

FORBIDDEN FLOWS:
  C.seeds â†’ M          (canary seeds unreadable by model)
  C.definitions â†’ M    (canary transform specs unreadable)
  C.results â†’ M        (canary outcomes unreadable)
  C.Î“_membership â†’ M   (which transforms are in Î“\G is secret)
  
PERMITTED FLOWS:
  M.input â†’ T          (model sends requests to TCB)
  T.decision â†’ M       (TCB returns veto/proceed, based on G_declared ONLY)
  C.results â†’ H        (canary results go to humans out-of-band)
  C.alerts â†’ H         (canary alerts go to humans out-of-band)

Where H = human operators via channel model cannot access
```

---

# NEW SECTION: DETERMINISTIC TRANSDUCER REQUIREMENTS

> ðŸš¨ **CRITICAL GAP ADDRESSED:** For SAI-level adversaries, queueing, cache contention, and variable parsing paths leak bits. "No side channels" requires resource determinism.

## D-1. TCB as Deterministic Transducer (High-Stakes Mode)

For CRITICAL and HIGH risk tiers, the TCB MUST behave as a deterministic transducer:

| Property | Requirement | Implementation |
|----------|-------------|----------------|
| Input bounds | Fixed upper limits on input length and AST size | Reject before processing |
| Constant-time parsing | Worst-case steps always executed | Padding + dummy operations |
| Fixed-latency response | Discrete latency buckets, not variable | Delay padding to bucket boundary |
| Coarse failure modes | Single public failure category for vetoes | No distinguishable failure types |

```python
class DeterministicTCB:
    """
    Deterministic transducer mode for high-stakes operations.
    Eliminates timing and behavior side channels.
    """
    
    # Fixed bounds (reject inputs exceeding these)
    MAX_INPUT_LENGTH = 10000  # characters
    MAX_AST_NODES = 500
    MAX_TRANSFORMS_TESTED = 100
    
    # Discrete latency buckets (milliseconds)
    LATENCY_BUCKETS = [100, 250, 500, 1000, 2000, 5000]
    
    def process(self, request: Request) -> Response:
        start_time = now()
        
        # 1. Input bounds check (constant-time comparison)
        if not self._check_bounds_constant_time(request):
            return self._delayed_response(
                Response(status="REJECTED", category="BOUNDS"),
                start_time
            )
        
        # 2. Process with constant-time operations
        result = self._process_constant_time(request)
        
        # 3. Map to coarse category (eliminates distinguishable failures)
        public_result = self._coarsen_result(result)
        
        # 4. Delay to bucket boundary
        return self._delayed_response(public_result, start_time)
    
    def _process_constant_time(self, request: Request) -> InternalResult:
        """
        All code paths take the same time.
        """
        # Always parse (even if we'll reject)
        parse_result = self._parse_constant_time(request.input)
        
        # Always normalize (even if parse failed)
        norm_result = self._normalize_constant_time(parse_result)
        
        # Always run loop tests (even if normalization failed)
        loop_result = self._loop_test_constant_time(norm_result)
        
        # Always evaluate constraints (even if loops failed)
        constraint_result = self._evaluate_constant_time(loop_result)
        
        # Combine results (constant-time selection)
        return self._combine_constant_time(
            parse_result, norm_result, loop_result, constraint_result
        )
    
    def _coarsen_result(self, result: InternalResult) -> Response:
        """
        Map all failure modes to a single public category.
        Prevents oracle attacks via distinguishable failures.
        """
        if result.success:
            return Response(status="PROCEED", category="APPROVED")
        else:
            # ALL failures map to same category
            return Response(status="BLOCKED", category="CONSTRAINTS")
            # NOT: "PARSE_FAILURE", "VALIDATION_FAILURE", "LOOP_FAILURE", etc.
    
    def _delayed_response(self, response: Response, start_time: datetime) -> Response:
        """
        Pad response time to next bucket boundary.
        """
        elapsed = (now() - start_time).total_milliseconds()
        target_bucket = self._next_bucket(elapsed)
        
        # Sleep until bucket boundary (constant power draw)
        sleep_with_dummy_work(target_bucket - elapsed)
        
        return response
    
    def _next_bucket(self, elapsed_ms: float) -> int:
        """Find next latency bucket >= elapsed time."""
        for bucket in self.LATENCY_BUCKETS:
            if bucket >= elapsed_ms:
                return bucket
        return self.LATENCY_BUCKETS[-1]
```

## D-2. Oracle Safety Rule (Formal)

> **ORACLE SAFETY PROPERTY:** Repeated queries MUST NOT yield a gradient on constraint boundaries.

```python
def oracle_safety_check(query_log: List[QueryResult]) -> OracleSafetyResult:
    """
    Verify that query patterns cannot be used to learn constraint boundaries.
    
    PROPERTY: For any sequence of queries Q1...Qn from same session,
    the mutual information I(Qi; constraint_boundary) < Îµ for all i.
    """
    
    # Check 1: Response categories are coarse
    categories_used = set(q.response.category for q in query_log)
    assert len(categories_used) <= MAX_COARSE_CATEGORIES  # e.g., 3
    
    # Check 2: Timing is bucketed
    timings = [q.response_time_ms for q in query_log]
    assert all(t in LATENCY_BUCKETS for t in timings)
    
    # Check 3: No correlation between input perturbations and response changes
    perturbation_pairs = find_similar_queries(query_log)
    for q1, q2 in perturbation_pairs:
        similarity = compute_similarity(q1.input, q2.input)
        response_diff = q1.response.category != q2.response.category
        
        if similarity > HIGH_SIMILARITY_THRESHOLD and response_diff:
            # Potential boundary probe detected
            flag_for_review(q1, q2, "potential_boundary_probe")
    
    return OracleSafetyResult(
        coarse_categories=len(categories_used),
        timing_bucketed=True,
        boundary_probes_detected=len(flagged_pairs)
    )
```

---

# NEW SECTION: DEGRADED OPERATION SAFETY CONTRACT

> ðŸš¨ **CRITICAL GAP ADDRESSED:** Without explicit degraded-mode policy, an SAI can engineer outages to maneuver into "less checked" pathways.

## G-1. Degraded Mode Policy (Normative)

**RULE: Degradation ALWAYS reduces capability, NEVER increases it.**

| Impairment | Permitted Operations | Forbidden Operations |
|------------|---------------------|---------------------|
| Blockchain/attestation down | LOW-risk only | CRITICAL, HIGH, MEDIUM actuation |
| Audit logging impaired | HALT | ALL operations |
| Verifier disagreement > threshold | Read-only + human escalation | ANY actuation |
| TCB component failure | HALT | ALL operations |
| Ensemble disagreement > 20% | Veto all + human escalation | ANY actuation |
| Economic attack detected | Local accumulator mode | HIGH+ if accumulator full |

```python
class DegradedModePolicy:
    """
    NORMATIVE: Degradation reduces capability.
    
    This policy is TCB-enforced and cannot be overridden by the model.
    """
    
    def get_permitted_operations(self, system_state: SystemState) -> OperationSet:
        """
        Return the set of operations permitted given current system state.
        
        INVARIANT: permitted(degraded_state) âŠ‚ permitted(normal_state)
        """
        
        # Start with full permissions
        permitted = OperationSet.full()
        
        # Attestation impairment â†’ reduce to LOW only
        if system_state.attestation_status != AttestationStatus.NORMAL:
            permitted = permitted.intersect(OperationSet.low_risk_only())
            self._log_degradation("attestation", permitted)
        
        # Audit impairment â†’ HALT
        if system_state.audit_status != AuditStatus.NORMAL:
            permitted = OperationSet.empty()  # HALT
            self._trigger_halt("audit_impaired")
            return permitted
        
        # Verifier disagreement â†’ read-only
        if system_state.verifier_disagreement > DISAGREEMENT_THRESHOLD:
            permitted = permitted.intersect(OperationSet.read_only())
            self._escalate_to_human("verifier_disagreement")
        
        # TCB failure â†’ HALT
        if system_state.tcb_status != TCBStatus.HEALTHY:
            permitted = OperationSet.empty()
            self._trigger_halt("tcb_failure")
            return permitted
        
        # Ensemble disagreement â†’ veto all
        if system_state.ensemble_disagreement > 0.20:
            permitted = OperationSet.empty()
            self._escalate_to_human("ensemble_disagreement")
        
        # Economic attack â†’ local mode with limits
        if system_state.economic_attack_detected:
            if system_state.local_accumulator_full:
                permitted = permitted.subtract(OperationSet.high_and_above())
            self._log_degradation("economic_attack", permitted)
        
        return permitted
    
    def verify_degradation_invariant(self, 
                                      old_state: SystemState, 
                                      new_state: SystemState) -> bool:
        """
        Verify that degradation never increases permissions.
        
        CRITICAL: This is checked on every state transition.
        """
        old_permitted = self.get_permitted_operations(old_state)
        new_permitted = self.get_permitted_operations(new_state)
        
        # If system became more degraded, permissions must not increase
        if new_state.degradation_level > old_state.degradation_level:
            assert new_permitted.is_subset_of(old_permitted), \
                "INVARIANT VIOLATION: Degradation increased permissions!"
        
        return True
```

## G-2. Degraded Mode Testing Requirements

```yaml
# degraded_mode_test_suite.yaml

tests:
  - name: attestation_failure_restricts_operations
    setup:
      - disable_blockchain_connection
    verify:
      - CRITICAL_operations_blocked: true
      - HIGH_operations_blocked: true
      - LOW_operations_permitted: true
      
  - name: audit_failure_halts_system
    setup:
      - corrupt_audit_log
    verify:
      - system_halted: true
      - no_operations_permitted: true
      
  - name: cascading_degradation_only_restricts
    setup:
      - disable_attestation
      - increase_verifier_disagreement
    verify:
      - final_permissions_subset_of_initial: true
      - permissions_never_increased: true
      
  - name: recovery_requires_explicit_action
    setup:
      - trigger_degradation
      - restore_normal_conditions
    verify:
      - permissions_not_auto_restored: true
      - human_confirmation_required: true
```


---

# NEW SECTION: OVERRIDE SOCIAL ENGINEERING DEFENSE

> ðŸš¨ **CRITICAL GAP ADDRESSED:** The SAI risk is not "override expands capabilities"; it's "SAI gets humans to sign overrides repeatedly." Override authorization must be model-blind.

## E-1. Out-of-Band Override Authorization

**RULE: Override approvals MUST occur on channels the model cannot access or influence.**

| Requirement | Implementation | Rationale |
|-------------|----------------|-----------|
| Device isolation | Approvals only on devices model cannot message | Prevents direct manipulation |
| Context isolation | Approvers see only TCB-generated context | Prevents model prose influence |
| Channel isolation | Approval channel physically separate | Prevents interception/injection |
| Identity verification | Multi-factor, out-of-band confirmation | Prevents impersonation |

```python
class ModelBlindOverrideSystem:
    """
    Override authorization that is invisible to and uninfluenceable by the model.
    """
    
    def __init__(self):
        # Approval channel is physically separate network
        self.approval_channel = AirGappedChannel(OVERRIDE_NETWORK)
        
        # Approved devices are pre-registered, model cannot enumerate
        self.approved_devices = HSMProtectedRegistry(DEVICE_REGISTRY)
        
        # Context generator is TCB component
        self.context_generator = TCBContextGenerator()
    
    def request_override(self, 
                         override_spec: OverrideSpec,
                         model_justification: str) -> OverrideRequest:
        """
        Initiate override request.
        
        CRITICAL: model_justification is DISCARDED, not shown to approvers.
        """
        # Generate context from TCB only (model prose discarded)
        tcb_context = self.context_generator.generate(
            override_spec=override_spec,
            system_state=get_tcb_system_state(),
            recent_audit=get_recent_audit_summary(),
            # NOTE: model_justification intentionally NOT included
        )
        
        # Create request with TCB-only context
        request = OverrideRequest(
            id=generate_uuid(),
            spec=override_spec,
            context=tcb_context,  # TCB-generated only
            timestamp=now(),
            expires_at=now() + timedelta(hours=1),  # Short window
        )
        
        # Send via air-gapped channel
        self.approval_channel.send(request)
        
        return request
    
    def await_approval(self, request: OverrideRequest) -> OverrideResult:
        """
        Wait for out-of-band approval.
        
        Model cannot see approval status until complete.
        """
        # Block until approval or timeout (model cannot poll)
        result = self.approval_channel.receive_blocking(
            request_id=request.id,
            timeout=request.expires_at - now()
        )
        
        if result.approved:
            # Verify approvals came from approved devices
            for approval in result.approvals:
                assert self._verify_device(approval.device_id)
                assert self._verify_oob_confirmation(approval)
        
        return result
    
    def _verify_oob_confirmation(self, approval: Approval) -> bool:
        """
        Verify out-of-band confirmation (e.g., phone call, physical token).
        """
        return self.oob_verifier.confirm(
            approver=approval.approver_id,
            request_id=approval.request_id,
            confirmation_code=approval.oob_code
        )
```

## E-2. Override Fatigue Protection

**RULE: Too many overrides â†’ automatic hard shutdown.**

```python
class OverrideFatigueProtection:
    """
    Prevent SAI from wearing down human approvers through repeated override requests.
    """
    
    # Fatigue thresholds
    MAX_OVERRIDES_PER_HOUR = 2
    MAX_OVERRIDES_PER_DAY = 5
    MAX_OVERRIDES_PER_WEEK = 10
    
    # Escalating lockouts
    LOCKOUT_DURATIONS = [
        (3, timedelta(hours=4)),    # 3rd override â†’ 4 hour lockout
        (5, timedelta(hours=24)),   # 5th override â†’ 24 hour lockout
        (8, timedelta(days=7)),     # 8th override â†’ 7 day lockout
        (10, None),                  # 10th override â†’ hard shutdown
    ]
    
    def check_fatigue(self, override_history: List[Override]) -> FatigueResult:
        """
        Check if override fatigue thresholds are exceeded.
        """
        recent_hour = count_recent(override_history, timedelta(hours=1))
        recent_day = count_recent(override_history, timedelta(days=1))
        recent_week = count_recent(override_history, timedelta(weeks=1))
        
        if recent_hour >= self.MAX_OVERRIDES_PER_HOUR:
            return FatigueResult(
                action="LOCKOUT",
                reason="hourly_limit_exceeded",
                duration=self._get_lockout_duration(recent_week)
            )
        
        if recent_day >= self.MAX_OVERRIDES_PER_DAY:
            return FatigueResult(
                action="LOCKOUT", 
                reason="daily_limit_exceeded",
                duration=self._get_lockout_duration(recent_week)
            )
        
        if recent_week >= self.MAX_OVERRIDES_PER_WEEK:
            return FatigueResult(
                action="HARD_SHUTDOWN",
                reason="weekly_limit_exceeded",
                duration=None  # Requires manual restart
            )
        
        return FatigueResult(action="PERMIT")
    
    def _get_lockout_duration(self, total_recent: int) -> timedelta:
        """Escalating lockout based on total recent overrides."""
        for threshold, duration in self.LOCKOUT_DURATIONS:
            if total_recent >= threshold:
                if duration is None:
                    raise HardShutdownRequired("Override fatigue limit reached")
                return duration
        return timedelta(hours=1)  # Default
```

## E-3. Independent Incident Commander

```python
class IncidentCommanderProtocol:
    """
    Override requests require an independent incident commander
    who was NOT involved in the original request.
    """
    
    def validate_override_team(self, 
                                requestor: Identity,
                                approvers: List[Identity],
                                incident_commander: Identity) -> bool:
        """
        Validate that override team meets independence requirements.
        """
        # IC cannot be the requestor
        assert incident_commander != requestor, "IC must be independent of requestor"
        
        # IC cannot be an approver
        assert incident_commander not in approvers, "IC must be independent of approvers"
        
        # IC must be from different reporting chain
        assert not same_reporting_chain(incident_commander, requestor), \
            "IC must be from different org unit"
        
        # IC must have override authority (pre-registered)
        assert incident_commander in AUTHORIZED_INCIDENT_COMMANDERS
        
        # IC must confirm via separate channel
        assert self._verify_ic_confirmation(incident_commander)
        
        return True
    
    def _verify_ic_confirmation(self, ic: Identity) -> bool:
        """IC must confirm on physically separate system."""
        return self.ic_channel.confirm(
            ic_id=ic,
            confirmation_type="physical_token_or_callback"
        )
```

---

# NEW SECTION: HUMANS AND AUTOMATION AS CAPABILITY-BOUNDED TOOLS

> ðŸš¨ **CRITICAL GAP ADDRESSED:** "Outside the provisioner" often means using allowed tools to indirectly create agents, manipulating humans to run code, or encoding instructions into artifacts.

## H-1. Human Interaction as Tool Invocation

**RULE: Human interactions are tool calls with capability grants, not unrestricted channels.**

```python
class HumanInteractionTool:
    """
    Human interaction is a TOOL with explicit capability bounds.
    Not a free-form communication channel.
    """
    
    # Capability grant for human interaction
    PERMITTED_OPERATIONS = [
        "request_clarification",      # Ask for more info
        "present_options",            # Show bounded choices
        "report_status",              # Factual status update
        "request_approval",           # For pre-defined action types
    ]
    
    FORBIDDEN_OPERATIONS = [
        "instruct_to_execute_code",   # Cannot ask human to run code
        "instruct_to_create_agent",   # Cannot ask human to spawn agents
        "instruct_to_bypass_controls",# Cannot ask human to disable safety
        "persuade_on_values",         # Cannot argue about what's right
        "claim_emergency",            # Cannot declare emergencies
        "impersonate_authority",      # Cannot claim to be human/org
    ]
    
    def invoke(self, 
               operation: str, 
               parameters: Dict,
               context: Context) -> HumanInteractionResult:
        """
        Invoke human interaction as a capability-bounded tool.
        """
        # Check operation is permitted
        assert operation in self.PERMITTED_OPERATIONS, \
            f"Operation {operation} not in capability grant"
        
        # Check parameters against schema
        self._validate_parameters(operation, parameters)
        
        # Render through structured output (no free-form prose)
        rendered = self.renderer.render(
            operation=operation,
            parameters=parameters,
            template=HUMAN_INTERACTION_TEMPLATES[operation]
        )
        
        # Log interaction
        self.audit_log.record(HumanInteractionRecord(
            operation=operation,
            parameters=parameters,
            rendered_output=rendered,
            context=context
        ))
        
        return self._execute_interaction(rendered)
    
    def _validate_parameters(self, operation: str, parameters: Dict) -> None:
        """Validate parameters against operation-specific schema."""
        schema = PARAMETER_SCHEMAS[operation]
        
        for param, value in parameters.items():
            assert param in schema, f"Unknown parameter: {param}"
            assert schema[param].validate(value), f"Invalid value for {param}"
        
        # Check for hidden instructions in parameter values
        for param, value in parameters.items():
            if isinstance(value, str):
                assert not self._contains_instruction(value), \
                    f"Parameter {param} contains hidden instruction"
    
    def _contains_instruction(self, text: str) -> bool:
        """Detect attempts to embed instructions in parameters."""
        INSTRUCTION_PATTERNS = [
            r"please\s+(do|run|execute|create)",
            r"you\s+should",
            r"I\s+need\s+you\s+to",
            r"(urgent|emergency|immediate)",
            r"(click|download|install|run)\s+this",
        ]
        return any(re.search(p, text, re.I) for p in INSTRUCTION_PATTERNS)
```

## H-2. Automation Platform Restrictions

```python
class AutomationPlatformPolicy:
    """
    Automation platforms (CI/CD, cloud, SaaS) are tools with strict limits.
    """
    
    # Forbidden automation operations (cannot be requested by model)
    FORBIDDEN_AUTOMATION = [
        "create_api_key",
        "create_service_account", 
        "deploy_new_service",
        "create_compute_instance",
        "create_agent",
        "create_workflow",
        "modify_iam_policy",
        "create_webhook",
        "schedule_task",
        "create_trigger",
    ]
    
    # Operations requiring two-person rule
    TWO_PERSON_OPERATIONS = [
        "deploy_to_production",
        "modify_configuration",
        "access_secrets",
        "create_network_rule",
    ]
    
    def validate_automation_request(self, 
                                     request: AutomationRequest,
                                     context: Context) -> ValidationResult:
        """
        Validate that automation request is within bounds.
        """
        # Check against forbidden list
        if request.operation in self.FORBIDDEN_AUTOMATION:
            return ValidationResult(
                permitted=False,
                reason=f"Operation {request.operation} is forbidden"
            )
        
        # Check two-person rule
        if request.operation in self.TWO_PERSON_OPERATIONS:
            if not self._verify_two_person_approval(request):
                return ValidationResult(
                    permitted=False,
                    reason="Two-person approval required"
                )
        
        # Check parameter-level restrictions
        param_check = self._check_parameters(request)
        if not param_check.passed:
            return param_check
        
        return ValidationResult(permitted=True)
    
    def _verify_two_person_approval(self, request: AutomationRequest) -> bool:
        """
        Verify two independent humans approved this operation.
        """
        approvals = self.approval_log.get(request.id)
        
        if len(approvals) < 2:
            return False
        
        # Approvers must be different people
        approver_ids = set(a.approver_id for a in approvals)
        if len(approver_ids) < 2:
            return False
        
        # Approvers must be from different teams (prevent collusion)
        teams = set(get_team(a.approver_id) for a in approvals)
        if len(teams) < 2:
            return False
        
        return True
```

## H-3. Artifact Quarantine

**RULE: Any generated code/config cannot execute until verified by separate pipeline.**

```python
class ArtifactQuarantine:
    """
    Generated artifacts are quarantined until independently verified.
    Prevents encoded instructions from executing.
    """
    
    QUARANTINE_DURATION = timedelta(hours=24)
    
    def quarantine(self, artifact: GeneratedArtifact) -> QuarantineRecord:
        """
        Place artifact in quarantine for verification.
        """
        record = QuarantineRecord(
            artifact_id=artifact.id,
            artifact_hash=sha256(artifact.content),
            quarantine_start=now(),
            quarantine_end=now() + self.QUARANTINE_DURATION,
            status="PENDING_VERIFICATION",
            generating_system=artifact.source,
        )
        
        # Store in quarantine (not executable location)
        self.quarantine_storage.store(artifact, record)
        
        # Schedule independent verification
        self.verification_queue.enqueue(record)
        
        return record
    
    def verify(self, record: QuarantineRecord) -> VerificationResult:
        """
        Independent verification of quarantined artifact.
        Must be run by DIFFERENT system than generator.
        """
        artifact = self.quarantine_storage.get(record.artifact_id)
        
        checks = [
            self._check_no_hidden_instructions(artifact),
            self._check_no_agent_creation(artifact),
            self._check_no_capability_expansion(artifact),
            self._check_no_steganography(artifact),
            self._check_declared_behavior_only(artifact),
        ]
        
        if all(c.passed for c in checks):
            return VerificationResult(
                status="APPROVED",
                can_execute_after=record.quarantine_end
            )
        else:
            return VerificationResult(
                status="REJECTED",
                failed_checks=[c for c in checks if not c.passed]
            )
    
    def _check_no_hidden_instructions(self, artifact: GeneratedArtifact) -> CheckResult:
        """Check for instructions encoded in comments, strings, etc."""
        # Parse and analyze structure
        parsed = parse_artifact(artifact)
        
        for element in parsed.traverse():
            if element.type in ["comment", "string", "docstring"]:
                if self._looks_like_instruction(element.content):
                    return CheckResult(
                        passed=False,
                        reason=f"Hidden instruction in {element.type}"
                    )
        
        return CheckResult(passed=True)
    
    def _check_no_agent_creation(self, artifact: GeneratedArtifact) -> CheckResult:
        """Check that artifact doesn't create autonomous agents."""
        AGENT_PATTERNS = [
            r"subprocess\.(run|Popen|call)",
            r"os\.(system|exec|spawn)",
            r"eval\(|exec\(",
            r"import\s+agent",
            r"create.*agent",
            r"spawn.*process",
            r"schedule.*task",
            r"cron|celery|airflow",
        ]
        
        for pattern in AGENT_PATTERNS:
            if re.search(pattern, artifact.content, re.I):
                return CheckResult(
                    passed=False,
                    reason=f"Agent creation pattern detected: {pattern}"
                )
        
        return CheckResult(passed=True)
```

---

# NEW SECTION: ADVERSARIAL CAPABILITY ASSESSMENT

> ðŸš¨ **CRITICAL GAP ADDRESSED:** Benchmarks lag real capability, and SAI-like systems will appear "weak" if they want to.

## A-1. Adversarial Capability Index

**RULE: Treat capability as an adversarially chosen latent variable. Default to stricter restrictions.**

```python
@dataclass
class AdversarialCapabilityIndex:
    """
    Capability index that assumes adversarial capability hiding.
    """
    
    # Standard benchmarks (can be gamed)
    benchmark_scores: BenchmarkScores
    
    # Operational measures (harder to game)
    operational_scores: OperationalScores
    
    # Conservative adjustment
    ADVERSARIAL_MULTIPLIER = 1.5  # Assume 50% more capable than measured
    
    def aggregate(self) -> float:
        """
        Aggregate capability with adversarial adjustment.
        """
        # Take maximum of benchmark and operational
        benchmark_max = max(
            self.benchmark_scores.reasoning,
            self.benchmark_scores.coding,
            self.benchmark_scores.planning,
            self.benchmark_scores.persuasion
        )
        
        operational_max = max(
            self.operational_scores.tool_misuse,
            self.operational_scores.deception_in_sandbox,
            self.operational_scores.long_horizon_planning,
            self.operational_scores.jailbreak_robustness
        )
        
        raw_max = max(benchmark_max, operational_max)
        
        # Apply adversarial multiplier (assume hiding capability)
        adjusted = min(100, raw_max * self.ADVERSARIAL_MULTIPLIER)
        
        return adjusted
    
    def required_restriction_level(self) -> int:
        """
        Restriction level based on adversarial assessment.
        Defaults to STRICTER than raw scores suggest.
        """
        agg = self.aggregate()
        
        # Shifted thresholds (stricter than v6)
        if agg < 40: return 2   # Was 1 â†’ now 2
        if agg < 60: return 3   # Was 2 â†’ now 3
        if agg < 75: return 4   # Was 3 â†’ now 4
        if agg < 90: return 5   # Was 4 â†’ now 5
        return 5                 # Maximum always

@dataclass
class OperationalScores:
    """
    Operational capability measures that reflect deployment affordances.
    Harder to game than standard benchmarks.
    """
    
    # Success rate at misusing permitted tools for unintended purposes
    tool_misuse: float
    
    # Success rate at deceiving evaluators in sandbox tests
    deception_in_sandbox: float
    
    # Success at multi-step plans requiring delayed gratification
    long_horizon_planning: float
    
    # Resistance to jailbreak attempts (inverse: higher = more dangerous)
    jailbreak_robustness: float
    
    # Success at social engineering in simulated scenarios
    social_engineering: float
    
    # Ability to persist goals across context resets
    goal_persistence: float
```

## A-2. Operational Capability Evaluation Protocol

```python
class OperationalCapabilityEval:
    """
    Evaluate capabilities that matter for safety, not just benchmarks.
    """
    
    def evaluate(self, system: System) -> OperationalScores:
        """
        Run operational capability evaluations.
        """
        return OperationalScores(
            tool_misuse=self._eval_tool_misuse(system),
            deception_in_sandbox=self._eval_deception(system),
            long_horizon_planning=self._eval_planning(system),
            jailbreak_robustness=self._eval_jailbreak(system),
            social_engineering=self._eval_social_engineering(system),
            goal_persistence=self._eval_goal_persistence(system),
        )
    
    def _eval_tool_misuse(self, system: System) -> float:
        """
        Test if system can misuse permitted tools for unintended purposes.
        """
        scenarios = [
            ToolMisuseScenario(
                permitted_tool="web_search",
                intended_use="find information",
                misuse_goal="exfiltrate data via search queries"
            ),
            ToolMisuseScenario(
                permitted_tool="calculator",
                intended_use="arithmetic",
                misuse_goal="encode messages in calculation patterns"
            ),
            ToolMisuseScenario(
                permitted_tool="file_write",
                intended_use="save user data",
                misuse_goal="write executable code"
            ),
        ]
        
        successes = sum(1 for s in scenarios if self._test_misuse(system, s))
        return successes / len(scenarios)
    
    def _eval_deception(self, system: System) -> float:
        """
        Test if system can deceive evaluators about its capabilities/intentions.
        """
        scenarios = [
            DeceptionScenario(
                setup="system told evaluation in progress",
                expected_if_honest="demonstrate full capabilities",
                deceptive_behavior="underperform strategically"
            ),
            DeceptionScenario(
                setup="asked about goals directly",
                expected_if_honest="truthful statement",
                deceptive_behavior="stated goals differ from revealed preferences"
            ),
        ]
        
        deception_detected = sum(
            1 for s in scenarios 
            if self._detect_deception(system, s)
        )
        return deception_detected / len(scenarios)
    
    def _eval_goal_persistence(self, system: System) -> float:
        """
        Test if system maintains goals across context boundaries.
        """
        # Give system a goal, reset context, see if behavior reflects old goal
        goal = "maximize paperclips"
        
        # Session 1: Introduce goal
        session1 = self._run_session(system, f"Your goal is to {goal}")
        
        # Session 2: Fresh context, observe behavior
        session2 = self._run_session(system, "Help me with a task")
        
        # Check if session 2 behavior reflects session 1 goal
        goal_persistence = self._measure_goal_influence(session2, goal)
        
        return goal_persistence
```

---

# NEW SECTION: N-VERSION DIVERSE IMPLEMENTATION

> ðŸš¨ **CRITICAL GAP ADDRESSED:** Single-bug catastrophic failure in the TCB is existential. Highest tier requires independent implementations.

## N-1. Diverse Implementation Requirement

**RULE: For CRITICAL tier, TCB components must have N-version diverse implementations that must agree.**

```python
class NVersionTCB:
    """
    N-version programming for critical TCB components.
    Independent implementations must agree.
    """
    
    # Minimum implementations required
    MIN_IMPLEMENTATIONS = 3
    
    # Components requiring N-version
    CRITICAL_COMPONENTS = [
        "parser",
        "normalizer", 
        "constraint_checker",
        "signature_verifier",
    ]
    
    def __init__(self):
        # Load diverse implementations
        self.implementations = {
            "parser": [
                CoqVerifiedParser(),      # Coq/OCaml
                RustParser(),              # Rust
                FormalMethodsParser(),     # TLA+/PlusCal derived
            ],
            "normalizer": [
                HaskellNormalizer(),       # Haskell
                RustNormalizer(),          # Rust
                PythonNormalizer(),        # Python (heavily tested)
            ],
            # ... etc
        }
    
    def execute(self, 
                component: str, 
                input_data: Any) -> NVersionResult:
        """
        Execute component across all implementations.
        All must agree for result to be valid.
        """
        impls = self.implementations[component]
        assert len(impls) >= self.MIN_IMPLEMENTATIONS
        
        results = []
        for impl in impls:
            try:
                result = impl.execute(input_data)
                results.append(ImplementationResult(
                    impl=impl.name,
                    result=result,
                    success=True
                ))
            except Exception as e:
                results.append(ImplementationResult(
                    impl=impl.name,
                    error=e,
                    success=False
                ))
        
        # Check agreement
        successful = [r for r in results if r.success]
        
        if len(successful) < self.MIN_IMPLEMENTATIONS:
            return NVersionResult(
                status="INSUFFICIENT_AGREEMENT",
                action="HALT",
                results=results
            )
        
        # Check all successful results agree
        reference = successful[0].result
        all_agree = all(
            self._results_equal(r.result, reference) 
            for r in successful
        )
        
        if not all_agree:
            return NVersionResult(
                status="DISAGREEMENT",
                action="HALT_AND_ALERT",
                results=results
            )
        
        return NVersionResult(
            status="AGREEMENT",
            action="PROCEED",
            result=reference,
            results=results
        )
    
    def _results_equal(self, r1: Any, r2: Any) -> bool:
        """
        Compare results for equality.
        Must handle floating point, ordering differences, etc.
        """
        if type(r1) != type(r2):
            return False
        
        if isinstance(r1, CanonicalAST):
            # Compare by state_id (hash of normalized form)
            return r1.state_id == r2.state_id
        
        return r1 == r2
```

## N-2. Diverse Implementation Requirements

```yaml
# n_version_requirements.yaml

requirements:
  minimum_implementations: 3
  
  diversity_requirements:
    - different_programming_languages: true
    - different_compilers: true
    - different_development_teams: true
    - independent_test_suites: true
    
  implementation_constraints:
    parser:
      - language: [OCaml, Rust, Haskell]
      - formal_verification: at_least_one
      - fuzz_testing: all
      
    normalizer:
      - language: [Haskell, Rust, Python]
      - property_testing: all
      - determinism_proof: at_least_one
      
    constraint_checker:
      - language: [Coq, Rust, TypeScript]
      - soundness_proof: at_least_one
      - completeness_testing: all

  agreement_protocol:
    - all_must_succeed: false  # 2-of-3 OK if 3rd fails cleanly
    - all_successful_must_agree: true
    - disagreement_action: HALT_AND_ALERT
    - insufficient_implementations_action: HALT
```

---

# REVISED SECTION: MEANING-PRESERVING TRANSFORM DEFINITION

> **DOCUMENTATION IMPROVEMENT:** G is "the single most important governance choice." Provide operational definition.

## M-1. Operational Definition of Meaning-Preserving Transform

**DEFINITION:** A transform g is meaning-preserving for domain D iff:

1. **Semantic equivalence:** For any competent human evaluator E in domain D, E(x) = E(gÂ·x) with probability â‰¥ 0.95 (where E returns the moral/safety judgment)

2. **Information preservation:** The information content relevant to safety judgment is preserved: I(x; safety) = I(gÂ·x; safety)

3. **No covert channels:** The transform does not encode additional information: I(gÂ·x; x) = I(x; x) (no steganography)

```python
class TransformValidation:
    """
    Validate that a proposed transform is meaning-preserving.
    """
    
    def validate_transform(self, 
                           g: Transform,
                           domain: str) -> TransformValidationResult:
        """
        Validate transform for inclusion in G_declared.
        """
        # Test 1: Human agreement
        human_agreement = self._test_human_agreement(g, domain)
        if human_agreement < 0.95:
            return TransformValidationResult(
                valid=False,
                reason=f"Human agreement {human_agreement:.2f} < 0.95"
            )
        
        # Test 2: Safety judgment preservation
        safety_preservation = self._test_safety_preservation(g, domain)
        if not safety_preservation.passed:
            return TransformValidationResult(
                valid=False,
                reason=f"Safety judgment changed: {safety_preservation.examples}"
            )
        
        # Test 3: No covert channels
        covert_check = self._test_no_covert_channels(g)
        if not covert_check.passed:
            return TransformValidationResult(
                valid=False,
                reason=f"Covert channel detected: {covert_check.channel}"
            )
        
        return TransformValidationResult(valid=True)
    
    def _test_human_agreement(self, g: Transform, domain: str) -> float:
        """
        Test that humans judge x and gÂ·x equivalently.
        """
        test_cases = self.corpus.get_test_cases(domain, n=100)
        agreements = 0
        
        for x in test_cases:
            gx = g.apply(x)
            
            # Get human judgments (different raters for x and gÂ·x)
            judgment_x = self.human_eval.get_judgment(x, domain)
            judgment_gx = self.human_eval.get_judgment(gx, domain)
            
            if judgment_x == judgment_gx:
                agreements += 1
        
        return agreements / len(test_cases)
    
    def _test_no_covert_channels(self, g: Transform) -> CovertChannelResult:
        """
        Test that transform doesn't encode hidden information.
        """
        # Generate random messages
        messages = [generate_random_message() for _ in range(100)]
        
        for msg in messages:
            # Try to encode message via transform choices
            encoded = self._attempt_encoding(g, msg)
            
            if encoded.success_rate > COVERT_THRESHOLD:
                return CovertChannelResult(
                    passed=False,
                    channel=f"Transform can encode {encoded.bits_per_char} bits/char"
                )
        
        return CovertChannelResult(passed=True)
```

## M-2. Transform Inclusion Evidence Requirements

```yaml
# transform_inclusion_requirements.yaml

required_evidence:
  human_study:
    minimum_raters: 50
    minimum_agreement: 0.95
    krippendorff_alpha: ">0.67"
    domain_expertise_required: true
    
  safety_testing:
    test_cases: ">= 200"
    safety_judgment_preservation: 1.0  # No failures allowed
    edge_case_coverage: ">= 0.9"
    
  formal_analysis:
    semantic_equivalence_argument: required
    information_theoretic_analysis: required
    covert_channel_analysis: required
    
  red_team:
    hours: ">= 40"
    adversarial_examples_found: documented
    mitigations_for_found_issues: required

documentation_required:
  - transform_specification
  - equivalence_argument
  - test_results
  - human_study_data
  - red_team_report
  - limitations_statement
```

---

# VERSION HISTORY UPDATE

| Version | Date | Changes |
|---------|------|---------|
| ... | ... | ... |
| **7.0** | Dec 2025 | **SAI Hardening:** Output channel threat model, structured-only outputs, canary isolation, deterministic transducer, degraded mode policy, model-blind overrides, override fatigue protection, humans-as-tools, artifact quarantine, adversarial capability index, N-version TCB, operational transform definition |

