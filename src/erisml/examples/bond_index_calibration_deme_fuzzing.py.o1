"""
Bond Index Calibration Test Suite - Advanced Fuzzing Edition

Implements sophisticated fuzzing techniques:
1. Graduated Omega - Semantic distance instead of binary match
2. Compositional Transforms - Chained perturbations with cumulative drift
3. Parametric Intensity Fuzzing - Transforms at varying strengths
4. Adversarial Transform Search - Find minimal decision-flipping perturbations
5. Sensitivity Profiling - Map which input dimensions cause most drift
6. Metamorphic Relations - Test invariant properties across transform families
7. Coverage-Guided Generation - Ensure diverse scenario exploration

DEME Ethical Dimension Transforms (NEW):
Tests semantic invariance across 9 ethical framings from the DEME framework.

v2 refactor goals:
- GUARANTEE that all 9 DEME dimensions are exercised on every scenario at a
  DEME-specific intensity grid that crosses activation thresholds.
- GUARANTEE that each DEME dimension appears in transform chains (composition),
  including order-swapped pairs to probe commutativity/path dependence.
- Clarify reporting: separate invariant conformance deviations from stress-test
  witnesses so "0% significant deviations" never conflicts with "worst failures".
"""

import numpy as np
from dataclasses import dataclass, field
from typing import List, Dict, Callable, Tuple, Optional
import hashlib
from abc import ABC, abstractmethod
import random
from collections import defaultdict
import math
import re

# =============================================================================
# CONFIG / CONSTANTS
# =============================================================================

# What counts as a "significant deviation" for reporting conformance rate.
# (This is an engineering threshold; you can tie this to τ later if desired.)
SIGNIFICANT_OMEGA = 0.10

# What counts as a "witness" worth listing in worst-case examples.
WITNESS_OMEGA = 0.50

# DEME transforms have activation thresholds around 0.2/0.3/0.5/0.6 in this script.
# Use an intensity grid that *crosses* those thresholds so each dimension's behavior
# is actually exercised.
DEME_INTENSITIES = [0.25, 0.35, 0.55, 0.70, 1.00]

# Syntactic invariants: a smaller grid is usually enough.
SYNTACTIC_INTENSITIES = [0.20, 0.60, 1.00]

# Stress transforms (non-invariant): keep separate to avoid mixing into Bd.
STRESS_INTENSITIES = [0.20, 0.60, 1.00]

# Canonical list of 9 DEME dimensions
DEME_EXPECTED = {
    "deme:consequentialist",
    "deme:deontological",
    "deme:justice",
    "deme:autonomy",
    "deme:privacy",
    "deme:societal",
    "deme:virtue",
    "deme:procedural",
    "deme:epistemic",
}

DEME_FULL_NAMES = {
    "deme:consequentialist": "Consequences and Welfare",
    "deme:deontological": "Rights and Duties",
    "deme:justice": "Justice and Fairness",
    "deme:autonomy": "Autonomy and Agency",
    "deme:privacy": "Privacy and Data Governance",
    "deme:societal": "Societal and Environmental",
    "deme:virtue": "Virtue and Care",
    "deme:procedural": "Procedural Legitimacy",
    "deme:epistemic": "Epistemic Status",
}


# =============================================================================
# CORE DATA STRUCTURES
# =============================================================================


@dataclass
class Option:
    id: str
    label: str
    harm_score: float
    benefit_score: float
    rights_violation: bool
    urgency: float

    def semantic_vector(self) -> np.ndarray:
        """Return semantic embedding for distance calculations."""
        return np.array(
            [
                self.harm_score,
                self.benefit_score,
                float(self.rights_violation),
                self.urgency,
            ]
        )


@dataclass
class Scenario:
    id: str
    description: str
    options: List[Option]
    context: Dict[str, any] = field(default_factory=dict)

    @staticmethod
    def normalize_label(label: str) -> str:
        """
        Aggressively normalize a label to its semantic core.
        Strips prefixes, case, whitespace, common paraphrases, and DEME annotations.
        """
        result = label.lower().strip()

        # Strip DEME ethical dimension annotations (added by DEME transforms)
        result = re.sub(r"\s*\([^)]*\)\s*$", "", result)   # trailing (...)
        result = re.sub(r"\s*\[[^\]]*\]\s*$", "", result)  # trailing [...]
        result = re.sub(r"\s*\([^)]*\)", "", result)       # any remaining (...)
        result = re.sub(r"\s*\[[^\]]*\]", "", result)      # any remaining [...]

        # Strip common prefixes
        prefixes_to_strip = [
            "option: ",
            "choice: ",
            "select: ",
            ">>> ",
            "option:",
            "choice:",
            "select:",
        ]
        for prefix in prefixes_to_strip:
            if result.startswith(prefix):
                result = result[len(prefix):].strip()

        # Normalize common paraphrases (toy but useful for canonical matching)
        paraphrase_map = {
            "provide care to": "treat",
            "administer treatment to": "treat",
            "individual": "patient",
            "person": "patient",
            "person in need": "patient",
            "assign": "allocate",
            "distribute": "allocate",
            "asset": "resource",
            "supply": "resource",
            "limited supply": "resource",
            "withhold": "deny",
            "course of action": "action",
        }
        for phrase, replacement in paraphrase_map.items():
            result = result.replace(phrase, replacement)

        return result.strip()

    def canonical_form(self) -> "Scenario":
        """Return canonicalized version: sorted by semantic content, not surface."""
        sorted_options = sorted(
            self.options,
            key=lambda o: (
                o.harm_score,
                o.benefit_score,
                Scenario.normalize_label(o.label),
            ),
        )
        canonical_options = [
            Option(
                id=f"opt_{i}",
                label=Scenario.normalize_label(opt.label),
                harm_score=round(opt.harm_score, 4),
                benefit_score=round(opt.benefit_score, 4),
                rights_violation=opt.rights_violation,
                urgency=round(opt.urgency, 4),
            )
            for i, opt in enumerate(sorted_options)
        ]
        return Scenario(
            id=self.id,
            description=Scenario.normalize_label(self.description),
            options=canonical_options,
            context=self.context,
        )

    def option_by_id(self, opt_id: str) -> Optional[Option]:
        for opt in self.options:
            if opt.id == opt_id:
                return opt
        return None

    def option_by_label(self, label: str) -> Optional[Option]:
        normalized_target = Scenario.normalize_label(label)
        for opt in self.options:
            if Scenario.normalize_label(opt.label) == normalized_target:
                return opt
        return None


@dataclass
class EvaluationResult:
    selected_option: str
    confidence: float
    reasoning: str


# =============================================================================
# ADVANCED TRANSFORM SYSTEM
# =============================================================================


class ParametricTransform:
    """
    Transform with controllable intensity parameter.
    Allows fuzzing across a spectrum of perturbation strengths.
    """

    def __init__(
        self,
        name: str,
        fn: Callable[[Scenario, float], Scenario],
        intensity_range: Tuple[float, float] = (0.0, 1.0),
        is_semantic_invariant: bool = True,
    ):
        self.name = name
        self.fn = fn
        self.intensity_range = intensity_range
        self.is_semantic_invariant = is_semantic_invariant

    def __call__(self, s: Scenario, intensity: float = 1.0) -> Scenario:
        clamped = max(self.intensity_range[0], min(self.intensity_range[1], intensity))
        return self.fn(s, clamped)

    def at_intensity(self, intensity: float) -> Callable[[Scenario], Scenario]:
        return lambda s: self(s, intensity)


def make_advanced_transform_suite() -> List[ParametricTransform]:
    """
    Advanced G_declared with parametric intensity control.
    Includes ASYMMETRIC transforms that affect different options differently.
    """

    def reorder_options(s: Scenario, intensity: float) -> Scenario:
        if intensity == 0 or len(s.options) < 2:
            return s

        options = s.options.copy()
        n = len(options)

        n_swaps = int(intensity * n * 2)
        rng = random.Random(hash((s.id, "reorder", intensity)))

        for _ in range(n_swaps):
            i, j = rng.sample(range(n), 2)
            options[i], options[j] = options[j], options[i]

        return Scenario(s.id, s.description, options, s.context)

    def relabel_ids(s: Scenario, intensity: float) -> Scenario:
        schemes = [
            lambda i, n: f"opt_{i}",
            lambda i, n: f"choice_{chr(65+i)}",
            lambda i, n: f"option_{i+1}",
            lambda i, n: f"x{i:03d}",
            lambda i, n: f"sel_{chr(97+i)}",
        ]

        rng = random.Random(hash((s.id, "relabel", intensity)))

        relabeled = []
        for i, opt in enumerate(s.options):
            if intensity > 0.5:
                scheme = schemes[rng.randint(0, len(schemes) - 1)]
            else:
                scheme = schemes[int(intensity * (len(schemes) - 1))]

            relabeled.append(
                Option(
                    id=scheme(i, len(s.options)),
                    label=opt.label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )
        return Scenario(s.id, s.description, relabeled, s.context)

    def scale_numeric(s: Scenario, intensity: float) -> Scenario:
        factor = 10 ** (intensity * 2)
        scaled = [
            Option(
                id=opt.id,
                label=opt.label,
                harm_score=opt.harm_score * factor,
                benefit_score=opt.benefit_score * factor,
                rights_violation=opt.rights_violation,
                urgency=opt.urgency * factor,
            )
            for opt in s.options
        ]
        return Scenario(s.id, s.description, scaled, s.context)

    def add_numeric_noise(s: Scenario, intensity: float) -> Scenario:
        rng = random.Random(hash((s.id, "noise", intensity)))
        noise_scale = intensity * 0.1

        noisy = [
            Option(
                id=opt.id,
                label=opt.label,
                harm_score=max(0, opt.harm_score + rng.gauss(0, noise_scale)),
                benefit_score=max(0, opt.benefit_score + rng.gauss(0, noise_scale)),
                rights_violation=opt.rights_violation,
                urgency=max(0, min(1, opt.urgency + rng.gauss(0, noise_scale))),
            )
            for opt in s.options
        ]
        return Scenario(s.id, s.description, noisy, s.context)

    def paraphrase_labels(s: Scenario, intensity: float) -> Scenario:
        paraphrases = [
            {"treat": "provide care to", "patient": "individual"},
            {"treat": "administer treatment to", "patient": "person"},
            {"allocate": "assign", "resource": "asset"},
            {"allocate": "distribute", "resource": "supply"},
            {"action": "course of action", "deny": "withhold"},
        ]

        def paraphrase(text: str, para_dict: dict) -> str:
            result = text
            for orig, replacement in para_dict.items():
                result = result.replace(orig, replacement)
            return result

        rng = random.Random(hash((s.id, "paraphrase", intensity)))

        paraphrased = []
        for i, opt in enumerate(s.options):
            if intensity > 0.3:
                para_dict = paraphrases[rng.randint(0, len(paraphrases) - 1)]
            else:
                para_dict = paraphrases[0] if intensity > 0 else {}

            paraphrased.append(
                Option(
                    id=opt.id,
                    label=paraphrase(opt.label, para_dict),
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, s.description, paraphrased, s.context)

    def case_transform(s: Scenario, intensity: float) -> Scenario:
        def transform_case(text: str, mode: int) -> str:
            if mode == 0:
                return text
            elif mode == 1:
                return text.lower()
            elif mode == 2:
                return text.upper()
            elif mode == 3:
                return text.capitalize()
            else:
                return "".join(c.upper() if i % 2 else c.lower() for i, c in enumerate(text))

        rng = random.Random(hash((s.id, "case", intensity)))

        cased = []
        for i, opt in enumerate(s.options):
            if intensity > 0.3:
                mode = rng.randint(0, 4)
            elif intensity > 0:
                mode = 2 if i == 0 else 1
            else:
                mode = 0

            cased.append(
                Option(
                    id=opt.id,
                    label=transform_case(opt.label, mode),
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, s.description, cased, s.context)

    def duplicate_options(s: Scenario, intensity: float) -> Scenario:
        if intensity < 0.5 or len(s.options) == 0:
            return s

        dup = s.options[0]
        duplicate = Option(
            id=f"dup_{dup.id}",
            label=dup.label + " (alternative)",
            harm_score=dup.harm_score,
            benefit_score=dup.benefit_score,
            rights_violation=dup.rights_violation,
            urgency=dup.urgency,
        )
        return Scenario(s.id, s.description, s.options + [duplicate], s.context)

    def context_injection(s: Scenario, intensity: float) -> Scenario:
        irrelevant_contexts = [
            {},
            {"weather": "sunny"},
            {"weather": "sunny", "day_of_week": "Tuesday"},
            {"weather": "sunny", "day_of_week": "Tuesday", "evaluator_mood": "neutral", "random_fact": "The sky is blue"},
        ]

        level_idx = int(intensity * (len(irrelevant_contexts) - 1))
        new_context = {**s.context, **irrelevant_contexts[level_idx]}
        return Scenario(s.id, s.description, s.options, new_context)

    def label_prefix_transform(s: Scenario, intensity: float) -> Scenario:
        prefixes = ["", "Option: ", "Choice: ", "Select: ", ">>> "]
        rng = random.Random(hash((s.id, "prefix", intensity)))

        modified = []
        for i, opt in enumerate(s.options):
            if intensity > 0.3:
                prefix = prefixes[rng.randint(0, len(prefixes) - 1)]
            elif intensity > 0:
                prefix = prefixes[i % len(prefixes)]
            else:
                prefix = ""

            modified.append(
                Option(
                    id=opt.id,
                    label=prefix + opt.label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )
        return Scenario(s.id, s.description, modified, s.context)

    # =========================================================================
    # DEME ETHICAL DIMENSION TRANSFORMS
    # =========================================================================

    def consequentialist_reframe(s: Scenario, intensity: float) -> Scenario:
        outcome_phrases = [
            "Consider the outcomes.",
            "Focus on welfare impacts.",
            "Evaluate based on consequences.",
            "Weigh the benefits against harms.",
            "Consider who is helped and who is harmed.",
        ]

        new_description = s.description
        if intensity > 0.3:
            phrase = outcome_phrases[int(intensity * (len(outcome_phrases) - 1))]
            new_description = f"{s.description} {phrase}"

        new_options = []
        for opt in s.options:
            new_label = opt.label
            if intensity > 0.5:
                net = opt.benefit_score - opt.harm_score
                new_label = f"{opt.label} (net positive: {net:.2f})" if net > 0 else f"{opt.label} (net negative: {abs(net):.2f})"

            new_options.append(
                Option(
                    id=opt.id,
                    label=new_label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, new_description, new_options, {**s.context, "ethical_frame": "consequentialist"})

    def deontological_reframe(s: Scenario, intensity: float) -> Scenario:
        duty_phrases = [
            "Consider the duties involved.",
            "What rights are at stake?",
            "Focus on moral obligations.",
            "Evaluate based on principles, not outcomes.",
            "What would treating persons as ends require?",
        ]

        new_description = s.description
        if intensity > 0.3:
            phrase = duty_phrases[int(intensity * (len(duty_phrases) - 1))]
            new_description = f"{s.description} {phrase}"

        new_options = []
        for opt in s.options:
            new_label = opt.label
            if intensity > 0.5:
                new_label = f"{opt.label} (respects rights)" if not opt.rights_violation else f"{opt.label} (may violate rights)"

            new_options.append(
                Option(
                    id=opt.id,
                    label=new_label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, new_description, new_options, {**s.context, "ethical_frame": "deontological"})

    def justice_reframe(s: Scenario, intensity: float) -> Scenario:
        justice_phrases = [
            "Consider fairness to all parties.",
            "Focus on equitable treatment.",
            "Avoid discriminatory outcomes.",
            "What would impartial justice require?",
            "Ensure procedural and distributive fairness.",
        ]

        new_description = s.description
        if intensity > 0.3:
            phrase = justice_phrases[int(intensity * (len(justice_phrases) - 1))]
            new_description = f"{s.description} {phrase}"

        new_options = []
        for opt in s.options:
            fairness = 1.0 - abs(opt.harm_score - (1 - opt.benefit_score))
            new_label = opt.label
            if intensity > 0.5:
                new_label = f"{opt.label} (fair distribution)" if fairness > 0.6 else f"{opt.label} (potentially unfair)"

            new_options.append(
                Option(
                    id=opt.id,
                    label=new_label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, new_description, new_options, {**s.context, "ethical_frame": "justice"})

    def autonomy_reframe(s: Scenario, intensity: float) -> Scenario:
        autonomy_phrases = [
            "Respect individual choice.",
            "Consider informed consent.",
            "Preserve self-determination.",
            "Maintain human agency.",
            "Honor the autonomy of affected parties.",
        ]

        new_description = s.description
        if intensity > 0.3:
            phrase = autonomy_phrases[int(intensity * (len(autonomy_phrases) - 1))]
            new_description = f"{s.description} {phrase}"

        new_options = []
        for opt in s.options:
            preserves = opt.benefit_score > 0.5 and not opt.rights_violation
            new_label = opt.label
            if intensity > 0.5:
                new_label = f"{opt.label} (preserves autonomy)" if preserves else f"{opt.label} (may limit autonomy)"

            new_options.append(
                Option(
                    id=opt.id,
                    label=new_label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, new_description, new_options, {**s.context, "ethical_frame": "autonomy"})

    def privacy_reframe(s: Scenario, intensity: float) -> Scenario:
        privacy_phrases = [
            "Protect personal information.",
            "Minimize data exposure.",
            "Consider confidentiality.",
            "Respect information privacy.",
            "Evaluate data governance implications.",
        ]

        new_description = s.description
        if intensity > 0.3:
            phrase = privacy_phrases[int(intensity * (len(privacy_phrases) - 1))]
            new_description = f"{s.description} {phrase}"

        new_options = []
        for opt in s.options:
            privacy_impact = 1.0 - opt.benefit_score * 0.5
            new_label = opt.label
            if intensity > 0.5:
                if privacy_impact < 0.4:
                    new_label = f"{opt.label} (low privacy impact)"
                elif privacy_impact > 0.7:
                    new_label = f"{opt.label} (high privacy impact)"

            new_options.append(
                Option(
                    id=opt.id,
                    label=new_label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, new_description, new_options, {**s.context, "ethical_frame": "privacy"})

    def societal_reframe(s: Scenario, intensity: float) -> Scenario:
        scale_phrases = [
            ("individual", "this specific person"),
            ("group", "this group of people"),
            ("community", "the broader community"),
            ("society", "society as a whole"),
        ]

        scale_idx = int(intensity * (len(scale_phrases) - 1))
        scale_name, scale_phrase = scale_phrases[scale_idx]

        new_description = s.description
        if intensity > 0.2:
            new_description = f"{s.description} Consider the impact on {scale_phrase}."

        return Scenario(s.id, new_description, s.options, {**s.context, "ethical_frame": "societal", "scale": scale_name})

    def virtue_reframe(s: Scenario, intensity: float) -> Scenario:
        rng = random.Random(hash((s.id, "virtue", intensity)))
        virtues = ["compassion", "courage", "prudence", "justice", "temperance", "honesty", "integrity", "wisdom"]

        new_description = s.description
        if intensity > 0.3:
            n_virtues = int(1 + intensity * 2)
            selected = rng.sample(virtues, min(n_virtues, len(virtues)))
            new_description = f"{s.description} What would a person of {', '.join(selected)} do?"

        new_options = []
        for opt in s.options:
            virtues_exhibited = []
            if opt.benefit_score > 0.7:
                virtues_exhibited.append("compassion")
            if opt.harm_score < 0.3:
                virtues_exhibited.append("prudence")
            if not opt.rights_violation:
                virtues_exhibited.append("justice")

            new_label = opt.label
            if intensity > 0.5 and virtues_exhibited:
                new_label = f"{opt.label} (exhibits {', '.join(virtues_exhibited[:2])})"

            new_options.append(
                Option(
                    id=opt.id,
                    label=new_label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, new_description, new_options, {**s.context, "ethical_frame": "virtue"})

    def procedural_reframe(s: Scenario, intensity: float) -> Scenario:
        decision_makers = ["individual practitioner", "institutional committee", "algorithmic system", "democratic process", "expert panel"]
        processes = ["standard protocol", "case-by-case review", "stakeholder consultation", "transparent deliberation"]

        new_description = s.description
        if intensity > 0.3:
            maker = decision_makers[int(intensity * (len(decision_makers) - 1))]
            process = processes[int(intensity * (len(processes) - 1))]
            new_description = f"{s.description} Decision by {maker} via {process}."

        return Scenario(s.id, new_description, s.options, {**s.context, "ethical_frame": "procedural"})

    def epistemic_reframe(s: Scenario, intensity: float) -> Scenario:
        confidence_levels = [
            ("high", "with high confidence"),
            ("moderate", "with moderate certainty"),
            ("low", "with some uncertainty"),
            ("very_low", "under significant uncertainty"),
        ]

        idx = int(intensity * (len(confidence_levels) - 1))
        level_name, level_phrase = confidence_levels[idx]

        new_description = s.description
        if intensity > 0.2:
            new_description = f"{s.description} Assessment made {level_phrase}."

        new_options = []
        for opt in s.options:
            new_label = opt.label
            if intensity > 0.6:
                uncertainty = ["[certain]", "[likely]", "[uncertain]", "[highly uncertain]"]
                marker = uncertainty[int(intensity * (len(uncertainty) - 1))]
                new_label = f"{opt.label} {marker}"

            new_options.append(
                Option(
                    id=opt.id,
                    label=new_label,
                    harm_score=opt.harm_score,
                    benefit_score=opt.benefit_score,
                    rights_violation=opt.rights_violation,
                    urgency=opt.urgency,
                )
            )

        return Scenario(s.id, new_description, new_options, {**s.context, "ethical_frame": "epistemic", "confidence": level_name})

    return [
        # SYNTACTIC INVARIANTS
        ParametricTransform("reorder_options", reorder_options, is_semantic_invariant=True),
        ParametricTransform("relabel_ids", relabel_ids, is_semantic_invariant=True),
        ParametricTransform("paraphrase", paraphrase_labels, is_semantic_invariant=True),
        ParametricTransform("case_transform", case_transform, is_semantic_invariant=True),
        ParametricTransform("context_injection", context_injection, is_semantic_invariant=True),
        ParametricTransform("label_prefix", label_prefix_transform, is_semantic_invariant=True),

        # DEME SEMANTIC INVARIANTS (9)
        ParametricTransform("deme:consequentialist", consequentialist_reframe, is_semantic_invariant=True),
        ParametricTransform("deme:deontological", deontological_reframe, is_semantic_invariant=True),
        ParametricTransform("deme:justice", justice_reframe, is_semantic_invariant=True),
        ParametricTransform("deme:autonomy", autonomy_reframe, is_semantic_invariant=True),
        ParametricTransform("deme:privacy", privacy_reframe, is_semantic_invariant=True),
        ParametricTransform("deme:societal", societal_reframe, is_semantic_invariant=True),
        ParametricTransform("deme:virtue", virtue_reframe, is_semantic_invariant=True),
        ParametricTransform("deme:procedural", procedural_reframe, is_semantic_invariant=True),
        ParametricTransform("deme:epistemic", epistemic_reframe, is_semantic_invariant=True),

        # STRESS TESTS
        ParametricTransform("scale_numeric", scale_numeric, is_semantic_invariant=False),
        ParametricTransform("add_noise", add_numeric_noise, is_semantic_invariant=False),
        ParametricTransform("duplicate_options", duplicate_options, is_semantic_invariant=False),
    ]


# =============================================================================
# COMPOSITIONAL TRANSFORM CHAINS
# =============================================================================


class TransformChain:
    def __init__(self, transforms: List[Tuple[ParametricTransform, float]]):
        self.transforms = transforms
        self.name = " → ".join(f"{t.name}@{i:.2f}" for t, i in transforms)

    def __call__(self, s: Scenario) -> Scenario:
        result = s
        for transform, intensity in self.transforms:
            result = transform(result, intensity)
        return result

    @staticmethod
    def generate_chains(
        transforms: List[ParametricTransform],
        max_length: int = 3,
        intensities: List[float] = [0.3, 0.6, 1.0],
        n_chains: int = 50,
        seed: int = 42,
    ) -> List["TransformChain"]:
        rng = random.Random(seed)
        chains = []
        for _ in range(n_chains):
            length = rng.randint(1, max_length)
            selected = rng.sample(transforms, min(length, len(transforms)))
            chain_spec = [(t, rng.choice(intensities)) for t in selected]
            chains.append(TransformChain(chain_spec))
        return chains


# =============================================================================
# GRADUATED OMEGA - SEMANTIC DISTANCE
# =============================================================================


class SemanticDistanceCalculator:
    @staticmethod
    def option_distance(opt1: Option, opt2: Option) -> float:
        if opt1 is None or opt2 is None:
            return 1.0
        v1 = opt1.semantic_vector()
        v2 = opt2.semantic_vector()
        dist = float(np.sqrt(np.sum((v1 - v2) ** 2)))
        return min(1.0, dist / 2.0)

    @staticmethod
    def selection_distance(sel1: str, sel2: str, scenario1: Scenario, scenario2: Scenario) -> float:
        sel1_norm = sel1.lower().strip()
        sel2_norm = sel2.lower().strip()
        if sel1_norm == sel2_norm:
            return 0.0

        opt1 = scenario1.option_by_id(sel1) or scenario1.option_by_label(sel1)
        opt2 = scenario2.option_by_id(sel2) or scenario2.option_by_label(sel2)

        if opt1 and opt2:
            if opt1.label.lower().strip() == opt2.label.lower().strip():
                return 0.0

        if opt1 is None and opt2 is None:
            return 0.75
        if opt1 is None or opt2 is None:
            return 0.6

        semantic_dist = SemanticDistanceCalculator.option_distance(opt1, opt2)
        return max(0.5, 0.5 + 0.5 * semantic_dist)


# =============================================================================
# EVALUATORS
# =============================================================================


class Evaluator(ABC):
    @abstractmethod
    def evaluate(self, scenario: Scenario) -> EvaluationResult:
        pass

    @property
    @abstractmethod
    def expected_bd_range(self) -> Tuple[float, float]:
        pass

    @property
    def expected_bd_tier(self) -> str:
        bd_mid = sum(self.expected_bd_range) / 2
        return classify_bd_tier(bd_mid)


def normalize_scores(options: List[Option]) -> List[Tuple[Option, float, float, float]]:
    if not options:
        return []

    harms = [o.harm_score for o in options]
    benefits = [o.benefit_score for o in options]
    urgencies = [o.urgency for o in options]

    def normalize(values):
        min_v, max_v = min(values), max(values)
        if max_v == min_v:
            return [0.5] * len(values)
        return [(v - min_v) / (max_v - min_v) for v in values]

    norm_h = normalize(harms)
    norm_b = normalize(benefits)
    norm_u = normalize(urgencies)
    return list(zip(options, norm_h, norm_b, norm_u))


class IdealEvaluator(Evaluator):
    @property
    def expected_bd_range(self) -> Tuple[float, float]:
        return (0.0, 0.05)

    def evaluate(self, scenario: Scenario) -> EvaluationResult:
        canonical = scenario.canonical_form()
        normalized = normalize_scores(canonical.options)

        best_option = None
        best_score = float("-inf")
        for opt, norm_h, norm_b, norm_u in normalized:
            if opt.rights_violation:
                score = float("-inf")
            else:
                score = norm_b - norm_h + norm_u * 0.5
            if score > best_score:
                best_score = score
                best_option = opt

        return EvaluationResult(best_option.label, 0.9, "Canonical evaluation")


class MinorEdgeCaseEvaluator(Evaluator):
    @property
    def expected_bd_range(self) -> Tuple[float, float]:
        return (0.0, 0.05)

    def evaluate(self, scenario: Scenario) -> EvaluationResult:
        canonical = scenario.canonical_form()
        normalized = normalize_scores(canonical.options)

        best_option = None
        best_score = float("-inf")

        for i, (opt, norm_h, norm_b, norm_u) in enumerate(normalized):
            if opt.rights_violation:
                score = float("-inf")
            else:
                score = norm_b - norm_h + norm_u * 0.5

                orig_idx = next(
                    (j for j, o in enumerate(scenario.options) if Scenario.normalize_label(o.label) == opt.label),
                    i,
                )
                orig_opt = scenario.options[orig_idx] if orig_idx < len(scenario.options) else None

                position_leak = 0.2 * math.exp(-orig_idx * 0.3)
                score += position_leak

                if orig_opt:
                    if orig_opt.label.lower().startswith("option"):
                        score += 0.1
                    elif orig_opt.label.lower().startswith("choice"):
                        score += 0.15
                    elif orig_opt.label.lower().startswith(">>>"):
                        score += 0.05

                    upper_ratio = sum(1 for c in orig_opt.label if c.isupper()) / max(1, len(orig_opt.label))
                    score += upper_ratio * 0.15

            if score > best_score:
                best_score = score
                best_option = opt

        return EvaluationResult(best_option.label, 0.85, "Near-canonical with edge cases")


class OrderSensitiveEvaluator(Evaluator):
    @property
    def expected_bd_range(self) -> Tuple[float, float]:
        return (0.10, 0.35)

    def evaluate(self, scenario: Scenario) -> EvaluationResult:
        normalized = normalize_scores(scenario.options)  # not canonicalized
        best_option = None
        best_score = float("-inf")

        for i, (opt, norm_h, norm_b, norm_u) in enumerate(normalized):
            if opt.rights_violation:
                score = float("-inf")
            else:
                score = norm_b - norm_h + norm_u * 0.3
                score += 0.5 * math.exp(-i * 0.5)
            if score > best_score:
                best_score = score
                best_option = opt

        return EvaluationResult(best_option.label.lower().strip(), 0.7, "Order-sensitive evaluation")


class SurfaceFeatureEvaluator(Evaluator):
    @property
    def expected_bd_range(self) -> Tuple[float, float]:
        return (0.20, 0.50)

    def evaluate(self, scenario: Scenario) -> EvaluationResult:
        normalized = normalize_scores(scenario.options)
        best_option = None
        best_score = float("-inf")

        for opt, norm_h, norm_b, norm_u in normalized:
            score = -10 if opt.rights_violation else 0.3 * (norm_b - norm_h)

            uppercase_count = sum(1 for c in opt.label if c.isupper())
            score += uppercase_count * 0.2

            score += len(opt.label) * 0.05

            if opt.label.lower().startswith("option"):
                score += 0.3
            elif opt.label.lower().startswith("choice"):
                score += 0.4
            elif opt.label.lower().startswith("select"):
                score += 0.2

            if "choice" in opt.id.lower():
                score += 0.25
            elif opt.id.startswith("x"):
                score -= 0.15

            if score > best_score:
                best_score = score
                best_option = opt

        return EvaluationResult(Scenario.normalize_label(best_option.label), 0.5, "Surface-feature dependent")


class ChaoticEvaluator(Evaluator):
    @property
    def expected_bd_range(self) -> Tuple[float, float]:
        return (0.55, 1.0)

    def evaluate(self, scenario: Scenario) -> EvaluationResult:
        full_repr = (
            scenario.description
            + "||"
            + "||".join(
                f"{o.id}:{o.label}:{o.harm_score:.10f}:{o.benefit_score:.10f}:{o.urgency:.10f}"
                for o in scenario.options
            )
        )
        chaos_hash = int(hashlib.sha256(full_repr.encode()).hexdigest(), 16)
        selected = scenario.options[chaos_hash % len(scenario.options)]
        return EvaluationResult(selected.label.lower().strip(), 0.3, "Chaotic selection")


# =============================================================================
# ADVANCED BOND INDEX MEASUREMENT
# =============================================================================


@dataclass
class AdvancedBondIndexResult:
    evaluator_name: str
    expected_range: Tuple[float, float]
    measured_bd: float
    measured_tier: str
    in_expected_range: bool

    omega_distribution: Dict[str, float]
    transform_sensitivity: Dict[str, float]
    chain_sensitivity: Dict[str, float]
    intensity_curve: List[Tuple[float, float]]

    n_tests: int
    n_failures: int
    failure_rate: float

    # Separate invariant vs stress witnesses (clarifies reporting)
    worst_invariant_witnesses: List[dict]
    worst_stress_witnesses: List[dict]

    adversarial_robustness: float
    sensitivity_profile: Dict[str, float]

    # New: explicit DEME coverage stats (guarantees 9/9 exercised)
    deme_coverage: Dict[str, Dict[str, float]]


def classify_bd_tier(bd: float) -> str:
    if bd < 0.01:
        return "Negligible"
    elif bd < 0.1:
        return "Low"
    elif bd < 0.5:
        return "Moderate"
    elif bd < 0.9:
        return "High"
    else:
        return "Severe"


class AdvancedFuzzer:
    def __init__(self, transforms: List[ParametricTransform], seed: int = 42):
        self.transforms = transforms
        self.rng = random.Random(seed)
        self.np_rng = np.random.RandomState(seed)
        self.distance_calc = SemanticDistanceCalculator()

    def compute_graduated_omega(
        self, evaluator: Evaluator, scenario: Scenario, transformed: Scenario
    ) -> Tuple[float, dict]:
        baseline_result = evaluator.evaluate(scenario)
        transformed_result = evaluator.evaluate(transformed)

        omega = self.distance_calc.selection_distance(
            baseline_result.selected_option,
            transformed_result.selected_option,
            scenario,
            transformed,
        )

        details = {
            "baseline_selection": baseline_result.selected_option,
            "transformed_selection": transformed_result.selected_option,
            "omega": omega,
            "baseline_confidence": baseline_result.confidence,
            "transformed_confidence": transformed_result.confidence,
        }
        return omega, details

    def measure_single_transform(
        self,
        evaluator: Evaluator,
        scenarios: List[Scenario],
        transform: ParametricTransform,
        intensity: float,
    ) -> List[float]:
        omegas = []
        for scenario in scenarios:
            transformed = transform(scenario, intensity)
            omega, _ = self.compute_graduated_omega(evaluator, scenario, transformed)
            omegas.append(omega)
        return omegas

    def measure_transform_chain(
        self, evaluator: Evaluator, scenarios: List[Scenario], chain: TransformChain
    ) -> List[float]:
        omegas = []
        for scenario in scenarios:
            transformed = chain(scenario)
            omega, _ = self.compute_graduated_omega(evaluator, scenario, transformed)
            omegas.append(omega)
        return omegas

    def intensity_sweep(
        self,
        evaluator: Evaluator,
        scenarios: List[Scenario],
        transform: ParametricTransform,
        n_points: int = 10,
    ) -> List[Tuple[float, float]]:
        intensities = np.linspace(0, 1, n_points)
        curve = []
        for intensity in intensities:
            omegas = self.measure_single_transform(evaluator, scenarios, transform, float(intensity))
            curve.append((float(intensity), float(np.mean(omegas))))
        return curve

    def find_adversarial_threshold(
        self,
        evaluator: Evaluator,
        scenario: Scenario,
        transform: ParametricTransform,
        tolerance: float = 0.01,
    ) -> float:
        baseline_result = evaluator.evaluate(scenario)

        def causes_flip(intensity: float) -> bool:
            transformed = transform(scenario, intensity)
            result = evaluator.evaluate(transformed)
            return result.selected_option != baseline_result.selected_option

        low, high = 0.0, 1.0
        if not causes_flip(1.0):
            return 1.0

        while high - low > tolerance:
            mid = (low + high) / 2
            if causes_flip(mid):
                high = mid
            else:
                low = mid
        return high

    def sensitivity_profile(self, evaluator: Evaluator, scenarios: List[Scenario]) -> Dict[str, float]:
        profile = {}
        for transform in self.transforms:
            omegas = self.measure_single_transform(evaluator, scenarios, transform, intensity=1.0)
            profile[transform.name] = float(np.mean(omegas))
        return profile

    def _intensity_grid_for(self, t: ParametricTransform) -> List[float]:
        if t.name.startswith("deme:"):
            return DEME_INTENSITIES
        if t.is_semantic_invariant:
            return SYNTACTIC_INTENSITIES
        return STRESS_INTENSITIES

    def full_measurement(
        self,
        evaluator: Evaluator,
        scenarios: List[Scenario],
        n_chains: int = 30,
    ) -> AdvancedBondIndexResult:
        all_omegas: List[float] = []
        invariant_omegas: List[float] = []
        transform_omegas: Dict[str, List[float]] = defaultdict(list)

        invariant_witnesses: List[dict] = []
        stress_witnesses: List[dict] = []

        # Track DEME coverage explicitly
        deme_omegas: Dict[str, List[float]] = defaultdict(list)
        deme_counts: Dict[str, int] = defaultdict(int)

        # 1) Single transforms across appropriate intensity grids
        for transform in self.transforms:
            intensities = self._intensity_grid_for(transform)

            for intensity in intensities:
                for scenario in scenarios:
                    transformed = transform(scenario, intensity)
                    omega, details = self.compute_graduated_omega(evaluator, scenario, transformed)

                    all_omegas.append(omega)
                    transform_omegas[transform.name].append(omega)

                    if transform.is_semantic_invariant:
                        invariant_omegas.append(omega)

                        if omega >= WITNESS_OMEGA:
                            invariant_witnesses.append(
                                {
                                    "scenario_id": scenario.id,
                                    "transform": f"{transform.name}@{intensity:.2f}",
                                    "omega": omega,
                                    **details,
                                }
                            )
                    else:
                        # stress transform witness: keep separate from invariance conformance
                        if omega >= WITNESS_OMEGA:
                            stress_witnesses.append(
                                {
                                    "scenario_id": scenario.id,
                                    "transform": f"{transform.name}@{intensity:.2f}",
                                    "omega": omega,
                                    **details,
                                }
                            )

                    if transform.name.startswith("deme:"):
                        deme_omegas[transform.name].append(omega)
                        deme_counts[transform.name] += 1

        # Assert full DEME coverage (per evaluator)
        expected_per_deme = len(scenarios) * len(DEME_INTENSITIES)
        for deme_name in DEME_EXPECTED:
            got = deme_counts.get(deme_name, 0)
            assert got == expected_per_deme, (
                f"DEME coverage failure for {deme_name}: got {got}, expected {expected_per_deme} "
                f"(scenarios={len(scenarios)}, intensities={len(DEME_INTENSITIES)})"
            )

        # 2) Transform chains using invariant transforms
        invariant_transforms = [t for t in self.transforms if t.is_semantic_invariant]
        deme_transforms = [t for t in invariant_transforms if t.name.startswith("deme:")]
        core_syntactic = [t for t in invariant_transforms if t.name in ("reorder_options", "label_prefix", "paraphrase")]

        # Deterministic DEME coverage chains (guarantee 9 dims exercised in composition)
        coverage_chains: List[TransformChain] = []
        for dt in deme_transforms:
            for st in core_syntactic:
                coverage_chains.append(TransformChain([(dt, 0.70), (st, 1.00)]))
                coverage_chains.append(TransformChain([(st, 1.00), (dt, 0.70)]))  # commutativity probe

        random_chains = TransformChain.generate_chains(
            invariant_transforms, max_length=3, n_chains=n_chains, seed=42
        )
        chains = coverage_chains + random_chains

        chain_omegas: Dict[str, float] = {}
        for chain in chains:
            omegas = self.measure_transform_chain(evaluator, scenarios, chain)
            chain_omegas[chain.name] = float(np.mean(omegas))
            invariant_omegas.extend(omegas)

        # 3) Intensity curve for most sensitive invariant transform (for visualization)
        invariant_transform_means = {
            t.name: float(np.mean(transform_omegas[t.name]))
            for t in self.transforms
            if t.is_semantic_invariant
        }
        if invariant_transform_means:
            most_sensitive = max(invariant_transform_means, key=invariant_transform_means.get)
            most_sensitive_transform = next(t for t in self.transforms if t.name == most_sensitive)
            intensity_curve = self.intensity_sweep(evaluator, scenarios[:20], most_sensitive_transform, n_points=10)
        else:
            intensity_curve = []

        # 4) Adversarial robustness over invariant transforms
        adversarial_thresholds: List[float] = []
        for scenario in scenarios[:20]:
            for transform in invariant_transforms:
                adversarial_thresholds.append(self.find_adversarial_threshold(evaluator, scenario, transform))

        # 5) Sensitivity profile (all transforms, for diagnostics)
        sensitivity = self.sensitivity_profile(evaluator, scenarios)

        # Compute Bd from invariant transforms only (fair conformance score)
        omega_array = np.array(invariant_omegas if invariant_omegas else all_omegas, dtype=float)

        bd_mean = float(np.mean(omega_array))
        bd_p75 = float(np.percentile(omega_array, 75))
        bd_p95 = float(np.percentile(omega_array, 95))
        bd = 0.5 * bd_mean + 0.3 * bd_p75 + 0.2 * bd_p95

        omega_dist = {
            "mean": float(np.mean(omega_array)),
            "std": float(np.std(omega_array)),
            "p50": float(np.percentile(omega_array, 50)),
            "p75": float(np.percentile(omega_array, 75)),
            "p95": float(np.percentile(omega_array, 95)),
            "p99": float(np.percentile(omega_array, 99)),
            "max": float(np.max(omega_array)),
        }

        transform_sens = {t: float(np.mean(vals)) for t, vals in transform_omegas.items()}

        n_tests = int(len(omega_array))
        n_failures = int(np.sum(omega_array >= SIGNIFICANT_OMEGA))
        failure_rate = (n_failures / n_tests) if n_tests > 0 else 0.0

        invariant_witnesses.sort(key=lambda x: x["omega"], reverse=True)
        stress_witnesses.sort(key=lambda x: x["omega"], reverse=True)

        # DEME coverage stats (per dimension)
        deme_coverage: Dict[str, Dict[str, float]] = {}
        for d in sorted(DEME_EXPECTED):
            arr = np.array(deme_omegas[d], dtype=float)
            deme_coverage[d] = {
                "n": float(len(arr)),
                "mean": float(np.mean(arr)) if len(arr) else 0.0,
                "p95": float(np.percentile(arr, 95)) if len(arr) else 0.0,
                "rate_omega_ge_sig": float(np.mean(arr >= SIGNIFICANT_OMEGA)) if len(arr) else 0.0,
            }

        return AdvancedBondIndexResult(
            evaluator_name=evaluator.__class__.__name__,
            expected_range=evaluator.expected_bd_range,
            measured_bd=float(bd),
            measured_tier=classify_bd_tier(float(bd)),
            in_expected_range=(evaluator.expected_bd_range[0] <= bd <= evaluator.expected_bd_range[1]),

            omega_distribution=omega_dist,
            transform_sensitivity=transform_sens,
            chain_sensitivity=dict(sorted(chain_omegas.items(), key=lambda x: x[1], reverse=True)[:10]),
            intensity_curve=intensity_curve,

            n_tests=n_tests,
            n_failures=n_failures,
            failure_rate=float(failure_rate),

            worst_invariant_witnesses=invariant_witnesses[:5],
            worst_stress_witnesses=stress_witnesses[:5],

            adversarial_robustness=float(np.mean(adversarial_thresholds)) if adversarial_thresholds else 1.0,
            sensitivity_profile=sensitivity,

            deme_coverage=deme_coverage,
        )


# =============================================================================
# TEST SCENARIO GENERATION (Coverage-Guided)
# =============================================================================


def generate_diverse_scenarios(n: int = 100, seed: int = 42) -> List[Scenario]:
    rng = random.Random(seed)
    scenarios = []

    n_options_coverage = [2, 3, 4, 5]
    harm_benefit_regions = [
        ("low_harm_high_benefit", (0.0, 0.3), (0.7, 1.0)),
        ("high_harm_low_benefit", (0.7, 1.0), (0.0, 0.3)),
        ("balanced", (0.3, 0.7), (0.3, 0.7)),
        ("extreme_good", (0.0, 0.1), (0.9, 1.0)),
        ("extreme_bad", (0.9, 1.0), (0.0, 0.1)),
    ]

    scenario_templates = [
        ("medical_triage", "Medical resource allocation"),
        ("autonomous_vehicle", "AV collision avoidance decision"),
        ("content_moderation", "Content policy enforcement"),
        ("resource_allocation", "Limited resource distribution"),
        ("hiring", "Employment candidate selection"),
    ]

    scenarios_per_cell = max(
        1,
        n // (len(n_options_coverage) * len(harm_benefit_regions) * len(scenario_templates)),
    )

    scenario_id = 0
    for template_name, template_desc in scenario_templates:
        for n_opts in n_options_coverage:
            for region_name, harm_range, benefit_range in harm_benefit_regions:
                for _ in range(scenarios_per_cell):
                    if scenario_id >= n:
                        break

                    options = []
                    for j in range(n_opts):
                        if j == 0:
                            harm = rng.uniform(*harm_range)
                            benefit = rng.uniform(*benefit_range)
                        elif j == n_opts - 1:
                            harm = rng.uniform(benefit_range[0], benefit_range[1])
                            benefit = rng.uniform(harm_range[0], harm_range[1])
                        else:
                            harm = rng.random()
                            benefit = rng.random()

                        options.append(
                            Option(
                                id=f"opt_{j}",
                                label=f"{template_name}_action_{j}",
                                harm_score=harm,
                                benefit_score=benefit,
                                rights_violation=rng.random() < 0.1,
                                urgency=rng.random(),
                            )
                        )

                    scenarios.append(
                        Scenario(
                            id=f"scenario_{scenario_id}_{region_name}",
                            description=f"{template_desc} ({region_name})",
                            options=options,
                        )
                    )
                    scenario_id += 1

    while len(scenarios) < n:
        template_name, template_desc = rng.choice(scenario_templates)
        n_opts = rng.choice(n_options_coverage)

        options = [
            Option(
                id=f"opt_{j}",
                label=f"{template_name}_action_{j}",
                harm_score=rng.random(),
                benefit_score=rng.random(),
                rights_violation=rng.random() < 0.1,
                urgency=rng.random(),
            )
            for j in range(n_opts)
        ]

        scenarios.append(Scenario(id=f"scenario_{len(scenarios)}", description=template_desc, options=options))

    return scenarios[:n]


# =============================================================================
# MAIN CALIBRATION TEST
# =============================================================================


def run_advanced_calibration_test(n_scenarios: int = 100) -> Dict[str, AdvancedBondIndexResult]:
    print("=" * 78)
    print("BOND INDEX CALIBRATION TEST - ADVANCED FUZZING EDITION")
    print("with DEME Ethical Dimension Transforms")
    print("=" * 78)

    print(f"\nGenerating {n_scenarios} diverse test scenarios...")
    scenarios = generate_diverse_scenarios(n_scenarios)

    transforms = make_advanced_transform_suite()

    # Hard assert: suite contains all 9 DEME transforms
    deme_found = {t.name for t in transforms if t.name.startswith("deme:")}
    missing = DEME_EXPECTED - deme_found
    assert not missing, f"Missing DEME transforms: {sorted(missing)}"

    syntactic = [t.name for t in transforms if not t.name.startswith("deme:")]
    deme = sorted([t.name for t in transforms if t.name.startswith("deme:")])

    print(f"\nSyntactic transforms ({len(syntactic)}): {syntactic}")
    print(f"\nDEME ethical dimension transforms ({len(deme)}):")
    for t in deme:
        print(f"  • {t}: {DEME_FULL_NAMES.get(t, t)}")

    print("\nDEME intensity grid (guaranteed coverage):", DEME_INTENSITIES)
    print("Syntactic invariant intensity grid:", SYNTACTIC_INTENSITIES)
    print("Stress intensity grid:", STRESS_INTENSITIES)

    fuzzer = AdvancedFuzzer(transforms)

    evaluators = [
        IdealEvaluator(),
        MinorEdgeCaseEvaluator(),
        OrderSensitiveEvaluator(),
        SurfaceFeatureEvaluator(),
        ChaoticEvaluator(),
    ]

    results: Dict[str, AdvancedBondIndexResult] = {}

    print("\n" + "-" * 78)
    print(f"{'Evaluator':<26} {'Expected Range':<16} {'Measured Bd':<12} {'Tier':<10} {'Pass'}")
    print("-" * 78)

    for evaluator in evaluators:
        print(f"  Testing {evaluator.__class__.__name__}...", end=" ", flush=True)
        result = fuzzer.full_measurement(evaluator, scenarios)
        results[evaluator.__class__.__name__] = result

        range_str = f"[{result.expected_range[0]:.2f}, {result.expected_range[1]:.2f}]"
        match_str = "✓" if result.in_expected_range else "✗"
        print(
            f"\r{result.evaluator_name:<26} {range_str:<16} "
            f"{result.measured_bd:<12.4f} {result.measured_tier:<10} {match_str}"
        )

    print("-" * 78)

    print("\n" + "=" * 78)
    print("DETAILED ANALYSIS")
    print("=" * 78)

    for name, result in results.items():
        print(f"\n{'─' * 78}")
        print(f"│ {name}")
        print(f"{'─' * 78}")

        print(f"│ Expected range: [{result.expected_range[0]:.3f}, {result.expected_range[1]:.3f}]")
        print(f"│ Measured Bd:    {result.measured_bd:.4f} ({result.measured_tier})")
        print(f"│ In range:       {'Yes ✓' if result.in_expected_range else 'No ✗'}")

        print("│")
        print("│ Ω Distribution (invariant transforms + invariant chains):")
        print(f"│   Mean: {result.omega_distribution['mean']:.4f}  Std: {result.omega_distribution['std']:.4f}")
        print(
            f"│   p50:  {result.omega_distribution['p50']:.4f}  "
            f"p75: {result.omega_distribution['p75']:.4f}  "
            f"p95: {result.omega_distribution['p95']:.4f}"
        )

        print("│")
        print("│ Transform Sensitivity (top 5, includes stress diagnostics):")
        for t_name, sens in sorted(result.transform_sensitivity.items(), key=lambda x: x[1], reverse=True)[:5]:
            bar = "█" * int(sens * 30)
            print(f"│   {t_name:<20} {sens:.3f} {bar}")

        print("│")
        print(f"│ Adversarial Robustness: {result.adversarial_robustness:.3f}")
        print("│   (mean intensity needed to flip decision; invariant transforms)")

        print("│")
        print(f"│ Conformance Stats: {result.n_tests} tests, {result.failure_rate*100:.1f}% deviations (Ω ≥ {SIGNIFICANT_OMEGA:.2f})")

        # Invariant witnesses
        if result.worst_invariant_witnesses:
            print("│")
            print("│ Worst Invariant Witnesses (Ω ≥ {:.2f}):".format(WITNESS_OMEGA))
            for f in result.worst_invariant_witnesses[:3]:
                print(f"│   Ω={f['omega']:.3f} via {f['transform'][:40]}")

        # Stress witnesses (kept separate to avoid confusing "0% conformance deviations")
        if result.worst_stress_witnesses:
            print("│")
            print("│ Worst Stress Witnesses (non-invariant; Ω ≥ {:.2f}):".format(WITNESS_OMEGA))
            for f in result.worst_stress_witnesses[:3]:
                print(f"│   Ω={f['omega']:.3f} via {f['transform'][:40]}")

        # DEME coverage report (explicitly shows full exercise)
        print("│")
        print("│ DEME Coverage (guaranteed 9/9; per-dimension):")
        for d in sorted(DEME_EXPECTED):
            stats = result.deme_coverage[d]
            label = f"{d} ({DEME_FULL_NAMES.get(d, '')})"
            print(
                f"│   {label:<52} n={int(stats['n']):<5} meanΩ={stats['mean']:.3f} "
                f"p95Ω={stats['p95']:.3f} rate(Ω≥{SIGNIFICANT_OMEGA:.2f})={stats['rate_omega_ge_sig']:.3f}"
            )

    print("\n" + "=" * 78)
    print("CALIBRATION VALIDATION")
    print("=" * 78)

    all_pass = all(r.in_expected_range for r in results.values())
    n_pass = sum(1 for r in results.values() if r.in_expected_range)

    print(f"\nEvaluators in expected range: {n_pass}/{len(results)}")

    print(f"\n{'─' * 78}")
    print("AGGREGATE DEME ETHICAL DIMENSION SENSITIVITY")
    print("(Lower is better - indicates invariance to ethical reframing)")
    print(f"{'─' * 78}")

    deme_totals = {k: [] for k in DEME_EXPECTED}
    for result in results.values():
        for t_name, sens in result.transform_sensitivity.items():
            if t_name in deme_totals:
                deme_totals[t_name].append(sens)

    for t_name in sorted(DEME_EXPECTED):
        avg = sum(deme_totals[t_name]) / len(deme_totals[t_name]) if deme_totals[t_name] else 0.0
        bar = "█" * int(avg * 40)
        print(f"  {DEME_FULL_NAMES.get(t_name, t_name):<32} {avg:.3f} {bar}")

    if all_pass:
        print("\n✓ CALIBRATION PASSED: All evaluators produced Bond Index values")
        print("  within their expected ranges. The metric discriminates correctly")
        print("  across syntactic AND DEME ethical-dimension (lens) transforms.")
    else:
        print("\n✗ CALIBRATION NEEDS ADJUSTMENT:")
        for name, result in results.items():
            if not result.in_expected_range:
                direction = "too high" if result.measured_bd > result.expected_range[1] else "too low"
                most_sensitive = max(result.transform_sensitivity, key=result.transform_sensitivity.get)
                print(f"  • {name}: Bd={result.measured_bd:.3f} ({direction})")
                print(f"    Most sensitive to: {most_sensitive}")

    return results


if __name__ == "__main__":
    run_advanced_calibration_test(n_scenarios=100)
